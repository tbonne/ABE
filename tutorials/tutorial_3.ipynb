{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABE Tutorial 3  \n",
    "## Actor-Critic Based Reinforcement Learning\n",
    "\n",
    "In this tutorial, we dive deeper into the reinforcement learning (RL) algorithm used in previous tutorials. We focus on the Actor-Critic method, which is a hybrid approach that combines ideas from value-based methods and policy-based methods. The actor network selects actions, while the critic network evaluates states, allowing the agent to learn both a policy and a value function simultaneously.\n",
    "\n",
    "**Key components:**\n",
    "\n",
    "- **Actor:**  \n",
    "  Takes the current state as input and produces raw scores (logits) for each possible action. When these logits are passed through a softmax, they yield a probability distribution over actions. The actor is responsible for learning the policy that selects actions expected to yield high rewards.\n",
    "\n",
    "- **Critic:**  \n",
    "  Evaluates the current state by outputting a single scalar value, representing the expected cumulative reward (or return) if the agent follows its policy from that state onward. This estimate is used as a baseline to compute the advantage, which guides the actor's updates.\n",
    "\n",
    "This architecture is used in many state-of-the-art deep RL methods, because the criticâ€™s evaluation stabilizes the policy updates by reducing variance in the gradient estimates.\n",
    "\n",
    "In this tutorial, you will learn how to:\n",
    "- Build an Actor-Critic neural network using PyTorch.\n",
    "- Create a custom policy with TD-learning.\n",
    "- Train and evaluate the agent in a Gymnasium environment (CartPole-v1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Walkthrough**: Actor-Critic Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off one of the main differences between the SARSA algorithm in the last tutorial and the Actor-Critic model we'll be learning here is that the Actor-Critic architecture has **two specialized components that work in tandem**:\n",
    "\n",
    "1. **Actor Branch**  \n",
    "   - **Input:**         The current state.  \n",
    "   - **Output:**        Raw scores (logits) for each possible action.  \n",
    "   - **Processing:**    A softmax converts these logits into a probability distribution over actions.  \n",
    "   - **Role:**          Learns the policy by favoring actions that are likely to yield higher rewards.\n",
    "\n",
    "2. **Critic Branch**  \n",
    "   - **Input:**     The same state.  \n",
    "   - **Output:**    A single scalar value estimating the expected return (or state value).  \n",
    "   - **Role:**      Provides a baseline to compute the advantage, i.e., the difference between the actual returns and the estimated value, which guides the policy update.\n",
    "\n",
    "Together, these branches enable the agent to both choose actions and assess their quality.\n",
    "\n",
    "Just like with the SARSA code let's create a new class:\n",
    "\n",
    "```python\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "    class ActorCriticNet(nn.Module):\n",
    "        \"\"\"\n",
    "        Actor-Critic network that outputs both the action logits (for the actor)\n",
    "        and the state value (for the critic).\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "            \"\"\"\n",
    "            Initialize the ActorCriticNet.\n",
    "\n",
    "            Parameters:\n",
    "                state_shape (tuple): Shape of the state space.\n",
    "                action_shape (int or tuple): Number of possible actions.\n",
    "                hidden_size (int): Number of units in hidden layers.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "\n",
    "            # Build the actor network: maps state -> action logits.\n",
    "\n",
    "            # Build the critic network: maps state -> scalar value.\n",
    "\n",
    "        def forward(self, obs, state=None, info={}):\n",
    "            \"\"\"\n",
    "            Perform a forward pass of the network.\n",
    "\n",
    "            Parameters:\n",
    "                obs (np.ndarray or torch.Tensor): Input observation.\n",
    "                state (optional): Not used here (for compatibility with recurrent models).\n",
    "                info (dict, optional): Additional information (unused).\n",
    "\n",
    "            Returns:\n",
    "                tuple: (action_logits, state_value)\n",
    "                    - action_logits (torch.Tensor): Logits for actions.\n",
    "                    - state_value (torch.Tensor): Estimated value of the state.\n",
    "            \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the parts let's fill in the code. Below we'll define some network layers for the critic and actor models.\n",
    "\n",
    "```python\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "    class ActorCriticNet(nn.Module):\n",
    "        \"\"\"\n",
    "        Actor-Critic network that outputs both the action logits (for the actor)\n",
    "        and the state value (for the critic).\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "            \"\"\"\n",
    "            Initialize the ActorCriticNet.\n",
    "\n",
    "            Parameters:\n",
    "                state_shape (tuple): Shape of the state space.\n",
    "                action_shape (int or tuple): Number of possible actions.\n",
    "                hidden_size (int): Number of units in hidden layers.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "\n",
    "            # Build the actor network: maps state -> action logits.\n",
    "            self.actor = nn.Sequential(\n",
    "                nn.Linear(np.prod(state_shape), hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.Linear(hidden_size, np.prod(action_shape))\n",
    "            )\n",
    "\n",
    "            # Build the critic network: maps state -> scalar value.\n",
    "            self.critic = nn.Sequential(\n",
    "                nn.Linear(np.prod(state_shape), hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.Linear(hidden_size, 1)\n",
    "            )\n",
    "```\n",
    "\n",
    "Next let's build the method that will get predictions from the actor-critic network models\n",
    "\n",
    "```python\n",
    "        def forward(self, obs, state=None, info={}):\n",
    "            \"\"\"\n",
    "            Perform a forward pass of the network.\n",
    "\n",
    "            Parameters:\n",
    "                obs (np.ndarray or torch.Tensor): Input observation.\n",
    "                state (optional): Not used here (for compatibility with recurrent models).\n",
    "                info (dict, optional): Additional information (unused).\n",
    "\n",
    "            Returns:\n",
    "                tuple: (action_logits, state_value)\n",
    "                    - action_logits (torch.Tensor): Logits for actions.\n",
    "                    - state_value (torch.Tensor): Estimated value of the state.\n",
    "            \"\"\"\n",
    "            # Convert observations to tensor if needed.\n",
    "            if isinstance(obs, np.ndarray):\n",
    "                obs = torch.tensor(obs, dtype=torch.float32)\n",
    "            action_logits = self.actor(obs)\n",
    "            state_value = self.critic(obs).squeeze(-1)\n",
    "            return action_logits, state_value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Custom Policy Definition\n",
    "\n",
    "In deep RL, the policy determines the action selection process. Here we implement an Advantage Actor-Critic (A2C) policy:\n",
    "- **Forward Method:** Computes action probabilities using the actor network.\n",
    "- **Learn Method:** Updates both actor and critic networks using TD-learning.  \n",
    "\n",
    "The critic computes a TD target and the advantage, which is used to update the actor via policy gradients.\n",
    "\n",
    "The advantage is calculated as:  \n",
    "$$\n",
    "\\text{advantage} = \\text{TD target} - \\text{state value},\n",
    "$$  \n",
    "where the TD target incorporates the observed reward and the discounted value of the next state. This formulation reduces the variance in policy updates.\n",
    "\n",
    "Below is the initial skeleton for our custom A2C policy.\n",
    "\n",
    "```python\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from tianshou.policy import BasePolicy\n",
    "    from tianshou.data import Batch\n",
    "\n",
    "    class A2CPolicy(BasePolicy):\n",
    "        \"\"\"\n",
    "        Advantage Actor-Critic (A2C) policy combining actor and critic networks.\n",
    "        Uses the critic's evaluation to compute an advantage for updating the actor.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, model, optim, action_space, gamma=0.99):\n",
    "            \"\"\"\n",
    "            Initialize the A2C policy.\n",
    "\n",
    "            Parameters:\n",
    "                model (nn.Module): The Actor-Critic network.\n",
    "                optim (torch.optim.Optimizer): Optimizer for training.\n",
    "                action_space: Environment's action space.\n",
    "                gamma (float): Discount factor.\n",
    "            \"\"\"\n",
    "\n",
    "        def forward(self, batch, state=None, **kwargs):\n",
    "            \"\"\"\n",
    "            Compute actions for given observations.\n",
    "\n",
    "            Parameters:\n",
    "                batch (Batch): Contains environment observations.\n",
    "                state (optional): Not used.\n",
    "                kwargs: Additional parameters.\n",
    "\n",
    "            Returns:\n",
    "                Batch: Contains chosen actions and the distribution.\n",
    "            \"\"\"\n",
    "\n",
    "        def learn(self, batch, **kwargs):\n",
    "            \"\"\"\n",
    "            Update the network using TD-learning.\n",
    "\n",
    "            Steps:\n",
    "            1. Compute the TD target: r + Î³(1-d) V(s')\n",
    "            2. Calculate the advantage: TD target - current state value.\n",
    "            3. Compute the actor (policy) loss and critic (value) loss.\n",
    "            4. Backpropagate and update the network weights.\n",
    "\n",
    "            Parameters:\n",
    "                batch (Batch): Batch of transitions.\n",
    "                kwargs: Additional parameters.\n",
    "\n",
    "            Returns:\n",
    "                dict: Loss values for monitoring.\n",
    "            \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the New Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the initialization of the policy first.\n",
    "\n",
    "When we initalize the training policy we'll provide information about:\n",
    "* the *model*, this will be used to choose actions given states\n",
    "* the *optim*, the optim (optimizer) will be used to update the model, i.e., it's how the agent will learn.\n",
    "* *gamma*, defines how the agent values future rewards vs. immediate rewards.\n",
    "* *Note:*, epsilon is no longer needed as our actor outputs a distribution of probabilities (logit scale) that defines the probability that the agent will select an action. So in this way our A2C algorithm already has exploration built in and does not need to take random actions.\n",
    "\n",
    "```python\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        from tianshou.policy import BasePolicy\n",
    "        from tianshou.data import Batch\n",
    "\n",
    "        class A2CPolicy(BasePolicy):\n",
    "        \"\"\"\n",
    "        Advantage Actor-Critic (A2C) policy combining actor and critic networks.\n",
    "        Uses the critic's evaluation to compute an advantage for updating the actor.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, model, optim, action_space, gamma=0.99):\n",
    "                \"\"\"\n",
    "                Initialize the A2C policy.\n",
    "\n",
    "                Parameters:\n",
    "                model (nn.Module): The Actor-Critic network.\n",
    "                optim (torch.optim.Optimizer): Optimizer for training.\n",
    "                action_space: Environment's action space.\n",
    "                gamma (float): Discount factor.\n",
    "                \"\"\"\n",
    "                super().__init__(action_space=action_space)\n",
    "                self.model = model\n",
    "                self.optim = optim\n",
    "                self.gamma = gamma\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add the forward method that takes the probabilities of action (on the logit scale) and uses those to choose what actions to take.\n",
    "\n",
    "* **Note**: The categorical distribution below is a good choice when the actions are discrete actions. We'll see how to modify this so that we can also work with continuous actions spaces.\n",
    "\n",
    "\n",
    "```python\n",
    "        def forward(self, batch, state=None, **kwargs):\n",
    "            \"\"\"\n",
    "            Compute actions for given observations.\n",
    "\n",
    "            Parameters:\n",
    "                batch (Batch): Contains environment observations.\n",
    "                state (optional): Not used.\n",
    "                kwargs: Additional parameters.\n",
    "\n",
    "            Returns:\n",
    "                Batch: Contains chosen actions and the distribution.\n",
    "            \"\"\"\n",
    "            logits, _ = self.model(batch.obs)\n",
    "            # Create a categorical distribution (suitable for discrete actions).\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            return Batch(act=action.cpu().numpy(), dist=dist)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we initialized our policy with a model, an optimizer, and some paramters, as well as setup how the agent will choose actions using the model, we need to think about how the agent will learn. This is an important step. We'll use TD-learning here to update the model so that it can predict the value of actions. It will do this in a few steps:\n",
    "\n",
    "* Estimate the current state value and the action probabilities.\n",
    "* Estimate the TD target: this is the expected value of the next state, action value.\n",
    "* Calculate the advantage: this measures how much better our predicted state values are when compared to what was seen before.\n",
    "* Use the difference between the current action probabilities and the advantage of each action to make the actor better. The ideal here is if the action probabilities exactly match the expected value returns of each action.\n",
    "* Use the difference between the estimated state value and the TD-target to make the critic better.\n",
    "* If we repeatedly do this the actor and critic network should make better estimates action probabilities, and state values.\n",
    "\n",
    "```python\n",
    "        def learn(self, batch, **kwargs):\n",
    "            \"\"\"\n",
    "            Update the network using TD-learning.\n",
    "\n",
    "            Steps:\n",
    "            1. Compute the TD target: r + Î³(1-d) V(s')\n",
    "            2. Calculate the advantage: TD target - current state value.\n",
    "            3. Compute the actor (policy) loss and critic (value) loss.\n",
    "            4. Backpropagate and update the network weights.\n",
    "\n",
    "            Parameters:\n",
    "                batch (Batch): Batch of transitions.\n",
    "                kwargs: Additional parameters.\n",
    "\n",
    "            Returns:\n",
    "                dict: Loss values for monitoring.\n",
    "            \"\"\"\n",
    "            logits, state_values = self.model(batch.obs)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            log_probs = dist.log_prob(batch.act)\n",
    "\n",
    "            # Compute TD target and advantage (without gradient tracking).\n",
    "            with torch.no_grad():\n",
    "                _, next_state_values = self.model(batch.obs_next)\n",
    "                td_target = batch.rew + self.gamma * (1 - batch.done) * next_state_values\n",
    "                advantage = td_target - state_values\n",
    "                # Normalize the advantage for stable learning.\n",
    "                advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "\n",
    "            # Compute policy (actor) loss and value (critic) loss.\n",
    "            policy_loss = -(log_probs * advantage.detach()).mean()\n",
    "            value_loss = nn.functional.mse_loss(state_values, td_target)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "            # Backpropagation with gradient clipping.\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optim.step()\n",
    "\n",
    "            return {\"loss\": loss.item(), \"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item()}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the policy and an actor/critic model, let's take a look at the full code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Full Implementation and Testing**: : Actor-Critic Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gone through the code piece-by-piece, let's take a look at that full code.\n",
    "\n",
    "This section combines the network and policy definitions, sets up the environment, runs a training loop, saves the model, and finally tests the trained agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preliminary Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import gymnasium as gym\n",
    "import pygame\n",
    "import torch\n",
    "from IPython.display import IFrame, display\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Local application/library-specific imports\n",
    "import tianshou as ts\n",
    "from tianshou.data import Collector, ReplayBuffer\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "# Timestamped ID for this run to avoid overwriting previous runs and to keep track of different runs\n",
    "agent_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Format: YYYYMMDD_HHMMSS\n",
    "\n",
    "# Setup directories for saving models and logs\n",
    "actor_critic_dir = f\"actor_critic_cartpole{agent_id}\"\n",
    "logs_dir = os.path.join(actor_critic_dir, \"logs\")\n",
    "models_dir = os.path.join(actor_critic_dir, \"models\")\n",
    "\n",
    "os.makedirs(actor_critic_dir, exist_ok=True) # Ensure the directory exists\n",
    "print(f\"All files for this run will be saved in the directory: {actor_critic_dir}\")\n",
    "os.makedirs(logs_dir, exist_ok=True) # Ensure the directory exists\n",
    "print(f\"Tensorboard logs will be saved in the directory: {logs_dir}\")\n",
    "os.makedirs(models_dir, exist_ok=True) # Ensure the directory exists\n",
    "print(f\"Models will be saved in the directory: {models_dir}\")\n",
    "\n",
    "# Create a logger\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter(logs_dir))\n",
    "print(f\"TensorBoard logs are being saved in: {logs_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Actor-Critic Neural Network (`ActorCriticNet`) Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ActorCriticNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic network that outputs both the action logits (for the actor)\n",
    "    and the state value (for the critic).\n",
    "\n",
    "    The network consists of two branches:\n",
    "    - Actor: Predicts the logits for each action.\n",
    "    - Critic: Estimates the scalar value of the current state.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "        \"\"\"\n",
    "        Initialize the ActorCriticNet.\n",
    "\n",
    "        Parameters:\n",
    "            state_shape (tuple): The shape of the state space.\n",
    "            action_shape (int or tuple): The number of possible actions.\n",
    "            hidden_size (int): Number of units in the hidden layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Build the actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, np.prod(action_shape))\n",
    "        )\n",
    "\n",
    "        # Build the critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the network.\n",
    "\n",
    "        Parameters:\n",
    "            obs (np.ndarray or torch.Tensor): Input observation from the environment.\n",
    "            state (optional): Unused parameter for compatibility with recurrent models.\n",
    "            info (dict, optional): Additional information (unused).\n",
    "\n",
    "        Returns:\n",
    "            tuple: (action_logits, state_value)\n",
    "                - action_logits (torch.Tensor): Logits for each action from the actor network.\n",
    "                - state_value (torch.Tensor): Estimated value of the state from the critic network.\n",
    "        \"\"\"\n",
    "        # Ensure the observation is a torch tensor\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        action_logits = self.actor(obs)\n",
    "        state_value = self.critic(obs).squeeze(-1)\n",
    "        return action_logits, state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Advantage Actor-Critic (A2C) Policy (`A2CPolicy`) Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tianshou.policy import BasePolicy\n",
    "from tianshou.data import Batch\n",
    "\n",
    "\n",
    "class A2CPolicy(BasePolicy):\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic (A2C) policy that combines actor and critic networks.\n",
    "    It samples actions based on the actor's logits and updates both networks using TD-learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, optim, action_space, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Initialize the A2C policy.\n",
    "\n",
    "        Parameters:\n",
    "            model (nn.Module): The Actor-Critic network.\n",
    "            optim (torch.optim.Optimizer): Optimizer for training the network.\n",
    "            action_space: The action space from the environment.\n",
    "            gamma (float): Discount factor for future rewards.\n",
    "        \"\"\"\n",
    "        super().__init__(action_space=action_space)\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, batch, state=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute the action for given observations.\n",
    "\n",
    "        Parameters:\n",
    "            batch (Batch): Contains observations from the environment.\n",
    "            state (optional): Not used in this implementation.\n",
    "            kwargs: Additional parameters.\n",
    "\n",
    "        Returns:\n",
    "            Batch: Contains the chosen actions and the associated distribution.\n",
    "        \"\"\"\n",
    "        logits, _ = self.model(batch.obs)\n",
    "        # Create a categorical distribution from the logits.\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return Batch(act=action.cpu().numpy(), dist=dist)\n",
    "\n",
    "    def learn(self, batch, **kwargs):\n",
    "        \"\"\"\n",
    "        Update the model based on a batch of experience.\n",
    "\n",
    "        The update involves:\n",
    "          1. Computing the TD target and advantage.\n",
    "          2. Calculating the loss for both actor and critic.\n",
    "          3. Performing a gradient descent step with gradient clipping.\n",
    "\n",
    "        Parameters:\n",
    "            batch (Batch): Batch of transitions containing observations, actions, rewards, etc.\n",
    "            kwargs: Additional parameters.\n",
    "\n",
    "        Returns:\n",
    "            dict: Contains overall loss, actor loss (policy_loss), and critic loss (value_loss).\n",
    "        \"\"\"\n",
    "        # Forward pass to get logits and state values.\n",
    "        logits, state_values = self.model(batch.obs)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(batch.act)\n",
    "\n",
    "        # Compute the TD target and advantage using no gradient tracking.\n",
    "        with torch.no_grad():\n",
    "            _, next_state_values = self.model(batch.obs_next)\n",
    "            td_target = batch.rew + self.gamma * (1 - batch.done) * next_state_values\n",
    "            advantage = td_target - state_values\n",
    "            # Normalize the advantage for stable training.\n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "\n",
    "        # Calculate the actor loss (policy loss) to encourage actions with higher advantage.\n",
    "        policy_loss = -(log_probs * advantage.detach()).mean()\n",
    "        # Calculate the critic loss as the mean squared error between predicted and target values.\n",
    "        value_loss = nn.functional.mse_loss(state_values, td_target)\n",
    "        # Combine both losses.\n",
    "        loss = policy_loss + value_loss\n",
    "\n",
    "        # Perform backpropagation.\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients to avoid exploding gradients.\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optim.step()\n",
    "\n",
    "        return {\"loss\": loss.item(), \"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create the Environment\n",
    "\n",
    "**Recall**:\n",
    "\n",
    "Here, we extract:\n",
    "- *State Shape:* The dimensionality of the observation space.\n",
    "- *Action Shape:* The number of discrete actions.\n",
    "- *Action Space:* For later use in the policy definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create the environment instance.\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Extract the shape of the observation space.\n",
    "state_shape = env.observation_space.shape\n",
    "\n",
    "# Extract the number of actions.\n",
    "action_shape = env.action_space.n\n",
    "\n",
    "# Store the action space for later use.\n",
    "action_space = env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating the Actor-Critic Network and Policy\n",
    "\n",
    "**Recall**:\n",
    "\n",
    "Now we instantiate our Actor-Critic network and set up the optimizer. The network is then used to initialize the A2C policy, which will drive our agentâ€™s learning. Notice how we pass the environment's action space for compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Select the appropriate device: CUDA (NVIDIA GPUs), MPS (Apple GPUs), or CPU.\n",
    "# AMD GPUs with ROCm support accessed using the 'cuda' device string.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate the Actor-Critic network.\n",
    "net = ActorCriticNet(state_shape, action_shape).to(device)\n",
    "\n",
    "# Define the optimizer with a small learning rate and weight decay.\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# Create the A2C policy using the network and optimizer.\n",
    "policy = A2CPolicy(model=net, optim=optimizer, action_space=action_space, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training the Agent\n",
    "\n",
    "**Recall**:\n",
    "\n",
    "The training loop uses Tianshouâ€™s components to collect experience and update the policy. We use:\n",
    "- *ReplayBuffer:* To store recent transitions.\n",
    "- *Collector:* To gather data from interactions with the environment.\n",
    "- *TensorBoard Logger:* For step-level and epoch-level metric tracking.\n",
    "\n",
    "During training, each step collects transitions and performs a TD-learning update.\n",
    "\n",
    "The following code implements the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.data import ReplayBuffer, Collector, Batch\n",
    "import tianshou as ts\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "def kill_port(port):\n",
    "    \"\"\"\n",
    "    Terminates any processes that are listening on the specified port.\n",
    "    Works on both Unix-based systems and Windows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.name == 'nt':\n",
    "            # Windows: Use netstat and taskkill to kill processes on the given port.\n",
    "            # The command below might fail (exit status 1) if no process is found.\n",
    "            cmd = f'for /f \"tokens=5\" %a in (\\'netstat -aon ^| findstr :{port}\\') do taskkill /F /PID %a'\n",
    "            result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Killed processes on port {port}.\")\n",
    "        else:\n",
    "            # Unix (Linux/Mac): Use lsof to find processes on the port and kill them.\n",
    "            cmd = f\"lsof -ti:{port} | xargs kill -9\"\n",
    "            result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Killed processes on port {port}.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # If the error message indicates that no process was found, we can ignore it.\n",
    "        if \"returned non-zero exit status 1\" in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Could not kill process on port {port}: {e}\")\n",
    "\n",
    "# Kill any processes on port 6006 to ensure it is free.\n",
    "kill_port(6006)\n",
    "\n",
    "# Clear previous TensorBoard sessions (cross-platform)\n",
    "tensorboard_info = os.path.join(tempfile.gettempdir(), \".tensorboard-info\")\n",
    "if os.path.exists(tensorboard_info):\n",
    "    shutil.rmtree(tensorboard_info)\n",
    "\n",
    "# Launch TensorBoard in the background on port 6006.\n",
    "tb_command = [\n",
    "    \"tensorboard\",\n",
    "    \"--logdir\", logs_dir,\n",
    "    \"--port\", \"6006\",\n",
    "    \"--host\", \"localhost\",\n",
    "    \"--reload_interval\", \"30\"\n",
    "]\n",
    "tb_process = subprocess.Popen(tb_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Allow time for TensorBoard to start and display its dashboard.\n",
    "time.sleep(5)\n",
    "display(IFrame(src=\"http://localhost:6006\", width=\"100%\", height=\"800\"))\n",
    "\n",
    "#------------------------------------------------------------------------------ \n",
    "\n",
    "# Hyperparameters for training.\n",
    "max_epoch = 3               # Total number of epochs for training.\n",
    "steps_per_epoch = 1000      # Number of training steps per epoch.\n",
    "keep_n_steps = 30           # Number of recent transitions to use for learning.\n",
    "\n",
    "# Initialize a ReplayBuffer to store recent transitions.\n",
    "buffer = ReplayBuffer(size=keep_n_steps)\n",
    "\n",
    "# Create collectors for training and testing.\n",
    "train_collector = Collector(policy, env, buffer)\n",
    "test_collector = Collector(policy, env)\n",
    "\n",
    "# Lists to store epoch summaries for later analysis.\n",
    "epoch_training_losses = []\n",
    "epoch_test_rewards = []\n",
    "epoch_durations = []\n",
    "\n",
    "global_start_time = time.time()  # Start the overall training timer.\n",
    "\n",
    "# Training loop with comprehensive progress tracking and logging.\n",
    "for epoch in range(max_epoch):\n",
    "    epoch_start_time = time.time()  # Timer for the current epoch.\n",
    "    train_collector.reset()         # Reset collector at the start of each epoch.\n",
    "    running_loss = 0.0              # Accumulate loss to compute the average loss.\n",
    "\n",
    "    # Set up a tqdm progress bar with dynamic post-fix metrics.\n",
    "    progress_bar = tqdm(\n",
    "        range(steps_per_epoch),\n",
    "        desc=f\"Epoch {epoch+1}/{max_epoch}\",\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "    \n",
    "    for step in progress_bar:\n",
    "        # Collect a fixed number of steps and store transitions in the buffer.\n",
    "        train_collector.collect(n_step=keep_n_steps)\n",
    "        # Retrieve the most recent transitions.\n",
    "        batch = train_collector.buffer[-keep_n_steps:]\n",
    "        \n",
    "        # Convert batch fields to torch tensors for compatibility.\n",
    "        batch.obs = torch.tensor(batch.obs, dtype=torch.float32)\n",
    "        batch.act = torch.tensor(batch.act, dtype=torch.long)\n",
    "        batch.rew = torch.tensor(batch.rew, dtype=torch.float32)\n",
    "        batch.done = torch.tensor(batch.done, dtype=torch.float32)\n",
    "        batch.obs_next = torch.tensor(batch.obs_next, dtype=torch.float32)\n",
    "        \n",
    "        # Update the policy using the collected batch and capture the loss.\n",
    "        learn_info = policy.learn(batch)\n",
    "        loss_val = learn_info.get(\"loss\", 0)\n",
    "        running_loss += loss_val\n",
    "        \n",
    "        global_step = epoch * steps_per_epoch + step\n",
    "        \n",
    "        # Log step-level metrics to TensorBoard.\n",
    "        logger.writer.add_scalar(\"Loss/train_step\", loss_val, global_step)\n",
    "        logger.writer.add_scalar(\"Loss/train_running_avg\", running_loss / (step + 1), global_step)\n",
    "        \n",
    "        # Flush logs periodically.\n",
    "        if step % 50 == 0:\n",
    "            logger.writer.flush()\n",
    "        \n",
    "        # Update progress bar with current metrics.\n",
    "        progress_bar.set_postfix({\n",
    "            \"Step\": f\"{step}/{steps_per_epoch}\",\n",
    "            \"Loss\": f\"{loss_val:07.3f}\",\n",
    "            \"AvgLoss\": f\"{running_loss / (step + 1):07.3f}\"\n",
    "        })\n",
    "        \n",
    "        # Print summary at every 25% of the epoch.\n",
    "        if step % (steps_per_epoch // 4) == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}, Step {step}/{steps_per_epoch}: \"\n",
    "                f\"Step Loss = {loss_val}, Running Avg Loss = {running_loss / (step + 1)}\"\n",
    "            )\n",
    "    \n",
    "    # Compute average loss over the epoch.\n",
    "    avg_loss = running_loss / steps_per_epoch\n",
    "\n",
    "    # Reset the test collector and evaluate the agent on 10 episodes.\n",
    "    test_collector.reset()\n",
    "    test_result = test_collector.collect(n_episode=10)\n",
    "    mean_reward = np.mean(test_result[\"rews\"])\n",
    "    std_reward = np.std(test_result[\"rews\"])\n",
    "    min_reward = np.min(test_result[\"rews\"])\n",
    "    p25_reward = np.percentile(test_result[\"rews\"], 25)\n",
    "    median_reward = np.median(test_result[\"rews\"])\n",
    "    p75_reward = np.percentile(test_result[\"rews\"], 75)\n",
    "    max_reward = np.max(test_result[\"rews\"])\n",
    "\n",
    "    # Log epoch-level metrics to TensorBoard.\n",
    "    logger.writer.add_scalar(\"Reward/test_avg\", mean_reward, epoch)\n",
    "    logger.writer.add_scalar(\"Loss/train_avg\", avg_loss, epoch)\n",
    "    logger.writer.flush()\n",
    "\n",
    "    # Calculate epoch elapsed time.\n",
    "    epoch_elapsed = time.time() - epoch_start_time\n",
    "    epoch_training_losses.append(avg_loss)\n",
    "    epoch_test_rewards.append(mean_reward)\n",
    "    epoch_durations.append(epoch_elapsed)\n",
    "\n",
    "    # Print detailed epoch summary.\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch+1} Summary:\\n\"\n",
    "        f\"  - Epoch Elapsed Time      : {epoch_elapsed} seconds\\n\"\n",
    "        f\"  - Steps Collected         : {steps_per_epoch}\\n\"\n",
    "        f\"  - Average Training Loss   : {avg_loss}\\n\"\n",
    "        f\"  - Mean Test Reward        : {mean_reward}\\n\"\n",
    "        f\"  - Std Test Reward         : {std_reward}\\n\"\n",
    "        f\"  - Min Test Reward         : {min_reward}\\n\"\n",
    "        f\"  - 25th Percentile Reward  : {p25_reward}\\n\"\n",
    "        f\"  - Median Test Reward      : {median_reward}\\n\"\n",
    "        f\"  - 75th Percentile Reward  : {p75_reward}\\n\"\n",
    "        f\"  - Max Test Reward         : {max_reward}\\n\"\n",
    "    )\n",
    "\n",
    "# Final flush and close the TensorBoard writer.\n",
    "logger.writer.close()\n",
    "\n",
    "# Calculate overall training statistics.\n",
    "total_elapsed = time.time() - global_start_time\n",
    "overall_avg_loss = np.mean(epoch_training_losses)\n",
    "overall_avg_reward = np.mean(epoch_test_rewards)\n",
    "total_epochs = len(epoch_durations)\n",
    "\n",
    "# Print overall training summary.\n",
    "print(\"\\nOverall Training Summary:\")\n",
    "print(f\"  - Total Epochs            : {total_epochs}\")\n",
    "print(f\"  - Overall Average Loss    : {overall_avg_loss}\")\n",
    "print(f\"  - Overall Average Reward  : {overall_avg_reward}\")\n",
    "print(f\"  - Total Elapsed Time      : {total_elapsed} seconds\")\n",
    "\n",
    "# Reiterate the final epoch's metrics.\n",
    "print(\"\\nFinal Epoch Summary:\")\n",
    "print(\n",
    "    f\"  - Epoch {total_epochs}:\\n\"\n",
    "    f\"      * Average Training Loss : {epoch_training_losses[-1]}\\n\"\n",
    "    f\"      * Average Test Reward   : {epoch_test_rewards[-1]}\\n\"\n",
    "    f\"      * Epoch Elapsed Time    : {epoch_durations[-1]} seconds\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it learn? Do you see rewards increasing? \n",
    "\n",
    "If so, let's save the model's state dictionary. This allows you to reload the trained agent later without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Save the model's state dictionary for future use.\n",
    "model_path = os.path.join(models_dir, f\"{actor_critic_dir}.pth\")\n",
    "torch.save(net.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluating the Trained Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the model, and watch what it learnt.\n",
    "\n",
    "Load in the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "model_path = os.path.join(models_dir, f\"{actor_critic_dir}.pth\")\n",
    "\n",
    "# Select the appropriate device: CUDA (NVIDIA GPUs), MPS (Apple GPUs), or CPU.\n",
    "# AMD GPUs with ROCm support accessed using the 'cuda' device string.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the trained policy\n",
    "loaded_net = ActorCriticNet(state_shape, action_shape).to(device)\n",
    "loaded_net.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an environment and build a policy based on our saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create an evaluation environment with rendering enabled.\n",
    "eval_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Create a new policy using the loaded network.\n",
    "loaded_policy = A2CPolicy(model=loaded_net, optim=optimizer, action_space=action_space, gamma=0.99)  # Set epsilon=0 for pure exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our agent in the environment.\n",
    "\n",
    "**Note**: You can change the number of episodes to watch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Batch\n",
    "\n",
    "num_episodes = 20     # Number of evaluation episodes.\n",
    "episode_rewards = []  # To store the total reward of each episode.\n",
    "episode_lengths = []  # To store the number of steps in each episode.\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    print(f\"Starting evaluation of episode {episode + 1}\")\n",
    "\n",
    "    while not done:\n",
    "        step_count += 1\n",
    "        # Create a batch from the current observation.\n",
    "        obs_batch = Batch(obs=[obs])\n",
    "        \n",
    "        # Get action from the policy (exploitation mode, no exploration noise).\n",
    "        with torch.no_grad():\n",
    "            action = loaded_policy.forward(obs_batch).act[0]\n",
    "        \n",
    "        # Take the action in the environment.\n",
    "        obs, reward, done, truncated, _ = eval_env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # End the episode if finished.\n",
    "        if done or truncated:\n",
    "            print(f\"Episode {episode + 1} ended with total reward: {total_reward} after {step_count} steps.\")\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(step_count)\n",
    "\n",
    "# Convert lists to numpy arrays for statistical calculations.\n",
    "episode_rewards = np.array(episode_rewards)\n",
    "episode_lengths = np.array(episode_lengths)\n",
    "\n",
    "# Compute and print comprehensive performance statistics.\n",
    "if episode_rewards.size > 0 and episode_lengths.size > 0:\n",
    "    count_rewards = len(episode_rewards)\n",
    "    mean_rewards = np.mean(episode_rewards)\n",
    "    std_rewards = np.std(episode_rewards)\n",
    "    min_rewards = np.min(episode_rewards)\n",
    "    p25_rewards = np.percentile(episode_rewards, 25)\n",
    "    median_rewards = np.median(episode_rewards)\n",
    "    p75_rewards = np.percentile(episode_rewards, 75)\n",
    "    max_rewards = np.max(episode_rewards)\n",
    "    \n",
    "    count_lengths = len(episode_lengths)\n",
    "    mean_lengths = np.mean(episode_lengths)\n",
    "    std_lengths = np.std(episode_lengths)\n",
    "    min_lengths = np.min(episode_lengths)\n",
    "    p25_lengths = np.percentile(episode_lengths, 25)\n",
    "    median_lengths = np.median(episode_lengths)\n",
    "    p75_lengths = np.percentile(episode_lengths, 75)\n",
    "    max_lengths = np.max(episode_lengths)\n",
    "    \n",
    "    print(\"\\nFinal Evaluation Performance Summary:\")\n",
    "    print(f\"Total Episodes Evaluated: {num_episodes}\\n\")\n",
    "    header = \"{:<22} {:>15} {:>20}\".format(\"Statistic\", \"Rewards\", \"Episode Lengths\")\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    print(\"{:<22} {:>15d} {:>20d}\".format(\"Count\", count_rewards, count_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Mean\", mean_rewards, mean_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Std Dev\", std_rewards, std_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Min\", min_rewards, min_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"25th Percentile\", p25_rewards, p25_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Median\", median_rewards, median_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"75th Percentile\", p75_rewards, p75_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Max\", max_rewards, max_lengths))\n",
    "else:\n",
    "    print(\"No performance data was collected. Please verify the Collector configuration.\")\n",
    "\n",
    "# Close the environment after evaluation.\n",
    "eval_env.close()\n",
    "print(\"Evaluation completed and environment closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Things to Try**\n",
    "\n",
    "Try changing the environment or changing the hyperparameters:\n",
    "\n",
    "- **Learning Rate:** Experiment with different learning rates. A rate that is too high may cause unstable updates; a rate that is too low may slow down learning.  \n",
    "- **Discount Factor ($\\gamma$):** Adjust the discount factor to balance the importance of immediate versus future rewards.  \n",
    "- **Network Architecture:** Modify the number of layers or hidden units to observe how it affects performance.  \n",
    "- **Environment Variations:** Test the agent in other environments to assess the robustness of the algorithm.\n",
    "\n",
    "Try altering some of these hyperparameters and see how that changes the ability of your agent to learn! Which hyperparameters work best?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABE_tutorial_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
