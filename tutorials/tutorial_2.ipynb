{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABE Tutorial 2\n",
    "## Value Based Reinforcement Learning\n",
    "\n",
    "In this second tutorial let's dive deeper into the RL algorithm that we used in the first tutorial. This time let's open up the policy and see how it learns in more detail.\n",
    "\n",
    "Steps:\n",
    "* Create a custom policy\n",
    "* Check how it updates based on temporal difference learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Custom Policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup a simple RL example\n",
    "\n",
    "Let's build a simple RL example based on what we learnt last tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import gymnasium as gym\n",
    "import pygame\n",
    "import torch\n",
    "from IPython.display import IFrame, display\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Local application/library-specific imports\n",
    "import tianshou as ts\n",
    "from tianshou.data import Collector, ReplayBuffer\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from tianshou.utils.net.common import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamped ID for this run to avoid overwriting previous runs and to keep track of different runs\n",
    "\n",
    "agent_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Format: YYYYMMDD_HHMMSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories for saving models and logs\n",
    "\n",
    "offline_policy_dir = f\"offline_policy_{agent_id}\"\n",
    "logs_dir = os.path.join(offline_policy_dir, \"logs\")\n",
    "models_dir = os.path.join(offline_policy_dir, \"models\")\n",
    "\n",
    "os.makedirs(offline_policy_dir, exist_ok=True) # Ensure the directory exists\n",
    "print(f\"All files for this run will be saved in the directory: {offline_policy_dir}\")\n",
    "os.makedirs(logs_dir, exist_ok=True) # Ensure the directory exists\n",
    "print(f\"Tensorboard logs will be saved in the directory: {logs_dir}\")\n",
    "os.makedirs(models_dir, exist_ok=True) # Ensure the directory exists\n",
    "print(f\"Models will be saved in the directory: {models_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logger\n",
    "\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter(logs_dir))\n",
    "print(f\"TensorBoard logs are being saved in: {logs_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the CartPole example an environment to train our agent in, and extract the observations, and the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment without opening a window.\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Reset the environment to obtain the initial observation and return a tuple (observation, info).\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Retrieve the observation and action spaces directly from the environment.\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "# Get the shape of the observation space (what the agent sees).\n",
    "state_shape = observation_space.shape\n",
    "\n",
    "# Determine the action space shape.\n",
    "# For discrete action spaces, use the number of available actions.\n",
    "if isinstance(action_space, gym.spaces.Discrete):\n",
    "    action_shape = action_space.n\n",
    "else:\n",
    "    action_shape = action_space.shape\n",
    "\n",
    "print(\"State shape:\", state_shape)\n",
    "print(\"Action shape:\", action_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the observation space, which describes what the agent can observe from the environment.\n",
    "print(f\"Observation space: {observation_space}\")\n",
    "\n",
    "# Print the action space, which describes the actions the agent can take in the environment.\n",
    "print(f\"Action space: {action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building our agent's brain. \n",
    "\n",
    "To start off let's build a neural network that take what the agent observes and converts that into actions. Then we'll add an optimizer to shift the weights in the network to better predict the value of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the network that maps observations to action values.\n",
    "\n",
    "# Select the appropriate device: CUDA (NVIDIA GPUs), MPS (Apple GPUs), or CPU.\n",
    "# AMD GPUs with ROCm support accessed using the 'cuda' device string.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# The Tianshou Net class (from tianshou.utils.net.common) includes several useful parameters:\n",
    "#   * input_size: Unpacked observation dimensions (e.g., 4 for CartPole).\n",
    "#   * output_size: Number of possible actions (e.g., 2 for CartPole).\n",
    "#   * hidden_sizes: List specifying the number of neurons in each hidden layer.\n",
    "#   * device: Device to run the network on (default: 'cpu'; set to 'cuda' if available).\n",
    "#   * activation: Activation function used between layers (default: torch.nn.ReLU).\n",
    "\n",
    "net = Net(\n",
    "    *state_shape,                    # Unpacked input dimensions (state size).\n",
    "    action_shape,                    # Output size: number of actions.\n",
    "    hidden_sizes=[64, 64],           # Hidden layer sizes; adjust for desired model capacity.\n",
    "    device=device,                   # Device to run the network; switch to 'cuda' if a GPU is available.\n",
    "    # activation=torch.nn.ReLU       # Activation function; change if another function is preferred.\n",
    ")\n",
    "\n",
    "# Print the network architecture to verify its structure.\n",
    "print(\"Network architecture:\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Adam optimizer for the network parameters.\n",
    "\n",
    "# torch.optim.Adam includes several parameters:\n",
    "#   * params: Iterable of network parameters to optimize.\n",
    "#   * lr: Learning rate (default here: 0.001). Tuning this affects convergence speed.\n",
    "#   * betas: Tuple for coefficients used in computing running averages (default: (0.9, 0.999)).\n",
    "#   * eps: Term added for numerical stability (default: 1e-08).\n",
    "#   * weight_decay: L2 penalty (default: 0).\n",
    "#   * amsgrad: Boolean flag to enable the AMSGrad variant (default: False).\n",
    "\n",
    "optim = torch.optim.Adam(\n",
    "    net.parameters(),                # Network parameters to be optimized.\n",
    "    lr=0.001,                        # Learning rate for updating parameters.\n",
    "    # betas=(0.9, 0.999),            # Coefficients for running averages; adjust for optimization behavior.\n",
    "    # eps=1e-08,                     # Epsilon for numerical stability; modify if experiencing issues.\n",
    "    # weight_decay=0,                # L2 regularization factor to prevent overfitting.\n",
    "    # amsgrad=False                  # Use AMSGrad variant if desired.\n",
    ")\n",
    "\n",
    "# Print the optimizer to verify its configuration.\n",
    "print(\"Optimizer configuration:\")\n",
    "print(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a network and an optimizer let's define a policy that will control how learning takes place.\n",
    "\n",
    "Let's use a pre-built policy first (we'll open it up and take a look inside... but first let's get this all working!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DQN policy\n",
    "\n",
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net,                          # Q-network approximating state-action values.\n",
    "    optim=optim,                        # Network optimizer.\n",
    "    discount_factor=0.9,                # Gamma. Balances immediate and future rewards. Values near 1 favor long-term rewards.\n",
    "    estimation_step=3,                  # Number of steps used in n-step return calculations. Increasing this incorporates more future reward but may also increase variance.\n",
    "    target_update_freq=320,             # Steps between updating the target network. A higher value results in more stable targets.\n",
    "    action_space=env.action_space,      # Informs the policy about available actions.\n",
    "    # reward_normalization=False,       # Flag to normalize rewards. Set to True if rewards have large variance or different scales.\n",
    ")\n",
    "\n",
    "# Print the policy to verify its configuration.\n",
    "print(\"Policy configuration:\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's setup a collector to feed observations to the policy as the agent interacts with it's environment.\n",
    "\n",
    "> We'll add a test collector that will run tests periodically to see how well our agent is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training and testing data collectors.\n",
    "# The Collector class in Tianshou collects experiences from interactions with the environment.\n",
    "\n",
    "# Its useful parameters include:\n",
    "#   * policy: The policy instance used to interact with the environment.\n",
    "#   * env: The environment instance from which to collect experiences.\n",
    "#   * buffer: The memory buffer that stores experiences; here, ReplayBuffer is used.\n",
    "#   * exploration_noise: (Optional) Adds noise for continuous control; not used for DQN.\n",
    "#   * preprocess_fn: (Optional) A function to preprocess data before storage.\n",
    "#   * Additional hidden parameters may involve trajectory recording and statistics.\n",
    "\n",
    "train_collector = Collector(\n",
    "    policy,                         # Policy used to select actions during data collection.\n",
    "    env,                            # Environment from which to collect experiences.\n",
    "    ReplayBuffer(10000),            # Replay buffer with a capacity of 10,000 experiences.\n",
    "    # exploration_noise=False,      # For DQN, epsilon-greedy is used; leave as default.\n",
    "    # preprocess_fn=None,           # Optional preprocessing function for modifying data before storage.\n",
    ")\n",
    "\n",
    "# Setup the test collector for evaluation.\n",
    "# Similar to the training collector but typically without additional exploration noise.\n",
    "test_collector = Collector(\n",
    "    policy,                         # Policy used for evaluation.\n",
    "    env,                            # Environment for testing the policy performance.\n",
    "    # exploration_noise=False,      # Typically disabled to ensure deterministic actions during testing.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have:\n",
    "\n",
    "1. An environment\n",
    "2. A Policy with a network model and an optimizer\n",
    "3. A collector to store the agent experiences\n",
    "\n",
    "We can now train our agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use an Off Policy Trainer for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.data import ReplayBuffer, Collector, Batch\n",
    "import tianshou as ts\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "def kill_port(port):\n",
    "    \"\"\"\n",
    "    Terminates any processes that are listening on the specified port.\n",
    "    Works on both Unix-based systems and Windows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.name == 'nt':\n",
    "            # Windows: Use netstat and taskkill to kill processes on the given port.\n",
    "            # The command below might fail (exit status 1) if no process is found.\n",
    "            cmd = f'for /f \"tokens=5\" %a in (\\'netstat -aon ^| findstr :{port}\\') do taskkill /F /PID %a'\n",
    "            result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Killed processes on port {port}.\")\n",
    "        else:\n",
    "            # Unix (Linux/Mac): Use lsof to find processes on the port and kill them.\n",
    "            cmd = f\"lsof -ti:{port} | xargs kill -9\"\n",
    "            result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Killed processes on port {port}.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # If the error message indicates that no process was found, we can ignore it.\n",
    "        if \"returned non-zero exit status 1\" in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Could not kill process on port {port}: {e}\")\n",
    "\n",
    "# Kill any processes on port 6006 to ensure it is free.\n",
    "kill_port(6006)\n",
    "\n",
    "# Clear previous TensorBoard sessions (cross-platform)\n",
    "tensorboard_info = os.path.join(tempfile.gettempdir(), \".tensorboard-info\")\n",
    "if os.path.exists(tensorboard_info):\n",
    "    shutil.rmtree(tensorboard_info)\n",
    "\n",
    "# Launch TensorBoard in the background on port 6006.\n",
    "tb_command = [\n",
    "    \"tensorboard\",\n",
    "    \"--logdir\", logs_dir,\n",
    "    \"--port\", \"6006\",\n",
    "    \"--host\", \"localhost\",\n",
    "    \"--reload_interval\", \"30\"\n",
    "]\n",
    "tb_process = subprocess.Popen(tb_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Allow time for TensorBoard to start and display its dashboard.\n",
    "time.sleep(5)\n",
    "display(IFrame(src=\"http://localhost:6006\", width=\"100%\", height=\"800\"))\n",
    "\n",
    "#------------------------------------------------------------------------------ \n",
    "\n",
    "# Configure and run the training loop using OffpolicyTrainer.\n",
    "\n",
    "# OffpolicyTrainer includes numerous parameters for fine-tuning the training process:\n",
    "#   * policy: The DQN policy instance to be trained.\n",
    "#   * train_collector: Collector that gathers training experiences.\n",
    "#   * test_collector: Collector that gathers evaluation data.\n",
    "#   * max_epoch: Maximum number of epochs for training (here: 10). Increase for longer training.\n",
    "#   * step_per_epoch: Number of environment steps per epoch (here: 10000). Adjust based on task complexity.\n",
    "#   * step_per_collect: Steps to collect between policy updates (here: 30). Lower values result in more frequent updates.\n",
    "#   * episode_per_test: Number of episodes used for evaluation in each test phase (here: 100).\n",
    "#   * batch_size: Mini-batch size for training updates (here: 64). Adjust based on available memory.\n",
    "#   * update_per_collect: Ratio of update steps per collected environment step (here: 1/10). Fine-tune for optimal learning.\n",
    "#   * train_fn: Function called during training at each epoch; here used to set epsilon for exploration.\n",
    "#   * test_fn: Function called during testing; typically sets a lower epsilon for evaluation.\n",
    "#   * stop_fn: Function to determine when to stop training (e.g., when a reward threshold is reached).\n",
    "#   * logger: Logger instance to record training progress and metrics.\n",
    "#   * save_checkpoint_fn: Function to save training checkpoints (default: None).\n",
    "\n",
    "trainer = OffpolicyTrainer(\n",
    "    policy=policy,                                                              # DQN policy to be trained.\n",
    "    train_collector=train_collector,                                            # Collector for training experiences.\n",
    "    test_collector=test_collector,                                              # Collector for evaluation.\n",
    "    max_epoch=5,                                                                # Total training epochs.\n",
    "    step_per_epoch=10000,                                                       # Environment steps per epoch.\n",
    "    step_per_collect=100,                                                       # Steps to collect between each update.\n",
    "    episode_per_test=10,                                                        # Episodes run during each evaluation phase.\n",
    "    batch_size=64,                                                              # Mini-batch size for training updates.\n",
    "    update_per_step=1 / 10,                                                     # Ratio of gradient updates per collected step.\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(0.1),                       # Function to adjust training parameters (e.g., epsilon).\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(0.05),                       # Function to adjust evaluation parameters.\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold,     # Stops training when the mean reward exceeds the threshold.\n",
    "    logger=logger,                                                              # Logger for tracking training progress.\n",
    "    # save_checkpoint_fn=None                                                   # Optional function to save training checkpoints.\n",
    ").run()\n",
    "\n",
    "# Print the full training summary.\n",
    "print(\"\\nTraining Summary:\\n\")\n",
    "for key, value in trainer.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Walkthrough**: SARSA Policy Using Temporal-Difference (TD) Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we build a custom policy that implements the SARSA algorithm using TD-learning.\n",
    "\n",
    "Instead of training offline on a fixed dataset, this approach learns continuously from the agent’s interactions with the environment. We define a custom policy class called `SARSAPolicy` that inherits from Tianshou’s `BasePolicy`.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Define a neural network (Q-network) for estimating state–action values.\n",
    "- Create a custom SARSA policy that selects actions using an epsilon-greedy strategy and updates the network parameters with TD-learning.\n",
    "- Build a Gymnasium environment, train the agent using a custom training loop, and then save and evaluate the trained model.\n",
    "\n",
    "For visualization and logging, we use TensorBoard again. Comments within the code explain each parameter and its impact, as well as how the deep learning methods are applied.\n",
    "\n",
    "The policy includes three key methods:\n",
    "\n",
    "- **`__init__`**: Initializes the policy by setting the model, optimizer, and hyperparameters.\n",
    "- **`forward`**: Receives the current observation and returns an action based on predicted state–action values.\n",
    "- **`learn`**: Updates the model using TD-learning by comparing the estimated state–action value with the TD target.\n",
    "\n",
    "```python\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from tianshou.policy import BasePolicy\n",
    "    from tianshou.data import Batch\n",
    "\n",
    "    # Custom SARSA policy class\n",
    "    class SARSAPolicy(BasePolicy):\n",
    "        \"\"\"\n",
    "        Custom SARSA policy implementing TD-learning for continuous online training.\n",
    "        \"\"\"\n",
    "        def __init__(self, model, optim, action_space, gamma=0.99, epsilon=0.1):\n",
    "            pass\n",
    "\n",
    "        def forward(self, batch: Batch, state=None, **kwargs) -> Batch:\n",
    "            \"\"\"\n",
    "            Forward pass to compute Q-values and select actions.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        def learn(self, batch: Batch, next_action: torch.Tensor, **kwargs) -> dict:\n",
    "            \"\"\"\n",
    "            Update the model using TD-learning.\n",
    "            \"\"\"\n",
    "            pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **SARSA Policy Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the initialization of the policy first.\n",
    "\n",
    "The `__init__` method of `SARSAPolicy` sets up the model, optimizer, and important hyperparameters such as the discount factor $ \\gamma $ and exploration probability $ \\epsilon $. This ensures that the policy has all the necessary components for both action selection and learning.\n",
    "\n",
    "When we initalize the training policy we'll provide information about:\n",
    "* the *model*, this will be used to choose actions given states\n",
    "* the *optim*, the optim (optimizer) will be used to update the model, i.e., it's how the agent will learn.\n",
    "* *gamma*, defines how the agent values future rewards vs. immediate rewards.\n",
    "* *epsilon*, defines the probability that the agent will select an action at random.\n",
    "\n",
    "```python\n",
    "        class SARSAPolicy(BasePolicy):\n",
    "        \"\"\"\n",
    "        Custom SARSA policy implementing TD-learning for continuous online training.\n",
    "\n",
    "        Args:\n",
    "                model (nn.Module): Neural network that predicts Q-values.\n",
    "                optim (torch.optim.Optimizer): Optimizer for training the model.\n",
    "                action_space (gym.Space): The environment's action space.\n",
    "                gamma (float, optional): Discount factor for future rewards. Default is 0.99.\n",
    "                epsilon (float, optional): Probability for epsilon-greedy exploration. Default is 0.1.\n",
    "        \"\"\"\n",
    "        def __init__(self, model, optim, action_space, gamma=0.99, epsilon=0.1):\n",
    "                super().__init__(action_space=action_space)\n",
    "                self.model = model\n",
    "                self.optim = optim\n",
    "                self.gamma = gamma\n",
    "                self.epsilon = epsilon\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Action Selection with the Forward Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `forward` method of `SARSAPolicy` computes Q-values for a batch of observations.\n",
    "- An epsilon-greedy strategy is implemented: the highest-value action is chosen most of the time, but a random action is occasionally selected for exploration.\n",
    "\n",
    "```python\n",
    "    def forward(self, batch: Batch, state=None, **kwargs) -> Batch:\n",
    "        \"\"\"\n",
    "        Forward pass to compute Q-values and select actions.\n",
    "\n",
    "        Uses an epsilon-greedy strategy: with probability epsilon, a random action is chosen;\n",
    "        otherwise, the action with the highest estimated Q-value is selected.\n",
    "\n",
    "        Args:\n",
    "            batch (Batch): A batch of observations.\n",
    "            state: (optional) The state information for recurrent models.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            Batch: A batch containing the chosen actions.\n",
    "        \"\"\"\n",
    "        # Predict Q-values from the model given current observations.\n",
    "        q_values, _ = self.model(batch.obs)\n",
    "        # Greedy action: select the action with the highest Q-value.\n",
    "        act = q_values.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        # Epsilon-greedy exploration: occasionally select a random action.\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            act = np.random.randint(0, q_values.shape[1], size=act.shape)\n",
    "\n",
    "        return Batch(act=act)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Learning with TD-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we initialized our policy with a model, an optimizer, and some parameters, as well as setup how the agent will choose actions using the model, we need to think about how the agent will learn. This is an important step. We'll use TD-learning here to update the model so that it can predict the value of actions. It will do this in a few steps:\n",
    "\n",
    "* Estimate the current state,action values: i.e., Q(s,a) or q_values.\n",
    "* Estimate the TD target: this is the expected value of the next state, action value.\n",
    "* Use the difference between the current q_value and td-target to update the neural network.\n",
    "* If we repeatedly do this the network should make better estimates of the q_values of state/action pairs.\n",
    "\n",
    "```python\n",
    "    def learn(self, batch: Batch, next_action: torch.Tensor, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Update the model using TD-learning.\n",
    "\n",
    "        Steps:\n",
    "        1. Compute current state-action values Q(s,a).\n",
    "        2. Estimate the TD target using the observed reward and the predicted next Q-values.\n",
    "        3. Calculate the mean squared error loss between the current Q-values and the TD target.\n",
    "        4. Perform a gradient step with gradient clipping to update the model.\n",
    "\n",
    "        Args:\n",
    "            batch (Batch): A batch containing observations, actions, rewards, etc.\n",
    "            next_action (torch.Tensor): The actions chosen for the next state.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the computed loss.\n",
    "        \"\"\"\n",
    "        # Get the predicted Q-values for the current state.\n",
    "        q_values, _ = self.model(batch.obs)\n",
    "\n",
    "        # Ensure actions are a tensor and have the correct shape.\n",
    "        batch_act = (\n",
    "            torch.tensor(batch.act, dtype=torch.long)\n",
    "            if not isinstance(batch.act, torch.Tensor)\n",
    "            else batch.act\n",
    "        )\n",
    "        # Gather Q-values corresponding to the chosen actions.\n",
    "        q_values = q_values.gather(1, batch_act.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute the TD target without tracking gradients.\n",
    "        with torch.no_grad():\n",
    "            next_q_values, _ = self.model(batch.obs_next)\n",
    "            next_q_values = next_q_values.gather(1, next_action.unsqueeze(1)).squeeze(1)\n",
    "            td_target = batch.rew + self.gamma * (1 - batch.done) * next_q_values\n",
    "\n",
    "        # Compute the mean squared error loss between current Q-values and the TD target.\n",
    "        loss = F.mse_loss(q_values, td_target)\n",
    "\n",
    "        # Reset gradients, backpropagate loss, clip gradients, and update weights.\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optim.step()\n",
    "\n",
    "        return {\"loss\": loss.item()}\n",
    "```\n",
    "\n",
    "In the `learn` method, the agent computes the temporal-difference (TD) target using:\n",
    "  \n",
    "  $$ TD_{target} = r + \\gamma (1 - d) Q(s', a') $$\n",
    "  \n",
    "  where $ r $ is the reward, $ \\gamma $ is the discount factor, $ d $ indicates if the state is terminal, and $ Q(s', a') $ is the next state–action value.\n",
    "The loss is computed using the mean squared error between the current Q-value estimate and the TD target.\n",
    "The optimizer then updates the model parameters, using gradient clipping to prevent large updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've defined the policy we have to define a neural network (i.e., the model) that the policy can use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model (QNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how to create an agent brain. The agent’s “brain” is a neural network model that estimates Q-values given the current state. To do this we'll create a new `QNet` class that extends PyTorch’s `nn.Module` and defines the network structure and forward pass.\n",
    "\n",
    "```python\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class QNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model to predict Q-values from state observations.\n",
    "    \"\"\"\n",
    "        def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "            pass\n",
    "\n",
    "        def forward(self, obs: torch.Tensor, state=None, info: dict = {}) -> tuple:\n",
    "            \"\"\"\n",
    "            Forward pass to compute Q-values.\n",
    "            \"\"\"\n",
    "            pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Neural Network (QNet) Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's take a look at the initialization first. When we initialize the agent's brain we'll provide information about:\n",
    "* the state_shape, this will be the size of the observation space, i.e., how many variables act as inputs for the agent?\n",
    "* the action_shape, the number of actions the agent can take.\n",
    "* hidden_size, defines the number of nodes in each hidden layer of the neural network.\n",
    "\n",
    "The `QNet` class constructs a network using fully connected layers. We flatten the input state (using $ \\prod(state\\_shape) $) to feed it into dense layers, and apply normalization layers to stabilize training.\n",
    "\n",
    "```python    \n",
    "    class QNet(nn.Module):\n",
    "        \"\"\"\n",
    "        Neural network model to predict Q-values from state observations.\n",
    "\n",
    "        Args:\n",
    "            state_shape (tuple): Shape of the input state.\n",
    "            action_shape (tuple): Shape of the action space.\n",
    "            hidden_size (int, optional): Number of units in the hidden layers. Default is 128.\n",
    "        \"\"\"\n",
    "        def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "            super().__init__()\n",
    "            # Build the neural network layers.\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(np.prod(state_shape), hidden_size),  # Dense layer to hidden_size units.\n",
    "                nn.ReLU(),                                      # Activation function.\n",
    "                nn.LayerNorm(hidden_size),                      # Normalization to stabilize training.\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.Linear(hidden_size, np.prod(action_shape))   # Output layer for Q-values.\n",
    "            )\n",
    "```\n",
    "\n",
    "The super().__init__() initializes the network based on PyTorch's nn.Module. Once this is done, let's add a neural network that the agent can use to predict the best action to take in the current context. Note: here we are only using the current state to make predictions about actions, we'l see later on how we can add in a trajectory of states from the recent past to help our agent make better decision in environments that are dynamic and stochastic.\n",
    "\n",
    "We'll cover how to build your own neural networks a little further along in the tutorials, but for now it's enough to know that we are building a neural network with 3 layers, using a RELU function to connect them, and doing some layerNormalization to avoid large changes in edge weights. The input will be the observed variables and the output will be the action taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Model Forward Pass**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at how to build the *forward* method to predict actions:\n",
    "\n",
    "```python\n",
    "    def forward(self, obs: torch.Tensor, state=None, info: dict = {}) -> tuple:\n",
    "        \"\"\"\n",
    "        Forward pass to compute Q-values.\n",
    "\n",
    "        Converts the input observation to a tensor if it is a numpy array, then passes it\n",
    "        through the network.\n",
    "\n",
    "        Args:\n",
    "            obs (torch.Tensor or np.ndarray): The observation/state input.\n",
    "            state: (optional) State information for recurrent networks.\n",
    "            info (dict, optional): Additional information (unused here).\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - q_values (torch.Tensor): Predicted Q-values for each action.\n",
    "                - state: Unchanged state (for compatibility with recurrent models).\n",
    "        \"\"\"\n",
    "        # Convert numpy arrays to torch tensors.\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        # Compute Q-values from the network.\n",
    "        q_values = self.net(obs)\n",
    "        return q_values, state\n",
    "```\n",
    "\n",
    "The `forward` method in `QNet` converts observations to a tensor (if necessary) and feeds them through the network to produce Q-values. This method is critical for both action selection and learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Full Implementation and Testing**: SARSA Policy Using Temporal-Difference (TD) Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gone through the code piece-by-piece, let's take a look at that full code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preliminary Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import gymnasium as gym\n",
    "import pygame\n",
    "import torch\n",
    "from IPython.display import IFrame, display\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Local application/library-specific imports\n",
    "import tianshou as ts\n",
    "from tianshou.data import Collector, ReplayBuffer\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "# Timestamped ID for this run to avoid overwriting previous runs and to keep track of different runs\n",
    "agent_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Format: YYYYMMDD_HHMMSS\n",
    "\n",
    "# Setup directories for saving models and logs\n",
    "sarsa_td_dir = f\"sarsa_td_cartpole_{agent_id}\"\n",
    "logs_dir = os.path.join(sarsa_td_dir, \"logs\")\n",
    "models_dir = os.path.join(sarsa_td_dir, \"models\")\n",
    "\n",
    "os.makedirs(sarsa_td_dir, exist_ok=True) # Ensure the directory exists\n",
    "print(f\"All files for this run will be saved in the directory: {sarsa_td_dir}\")\n",
    "os.makedirs(logs_dir, exist_ok=True) # Ensure the directory exists\n",
    "print(f\"Tensorboard logs will be saved in the directory: {logs_dir}\")\n",
    "os.makedirs(models_dir, exist_ok=True) # Ensure the directory exists\n",
    "print(f\"Models will be saved in the directory: {models_dir}\")\n",
    "\n",
    "# Create a logger\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter(logs_dir))\n",
    "print(f\"TensorBoard logs are being saved in: {logs_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SARSA Policy (`SARSAPolicy`) Full Code\n",
    "\n",
    "The `SARSAPolicy` class extends Tianshou’s `BasePolicy` and implements an epsilon-greedy strategy. It also performs the learning update using TD-learning by computing the loss between the current Q-values and the TD target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tianshou.policy import BasePolicy\n",
    "from tianshou.data import Batch\n",
    "\n",
    "class SARSAPolicy(BasePolicy):\n",
    "    \"\"\"\n",
    "    Custom SARSA policy implementing TD-learning.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Neural network that predicts Q-values.\n",
    "        optim (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
    "        action_space (gym.Space): The action space of the environment.\n",
    "        gamma (float, optional): Discount factor for future rewards. Default is 0.99.\n",
    "        epsilon (float, optional): Exploration probability for epsilon-greedy action selection. Default is 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, optim, action_space, gamma=0.99, epsilon=0.1):\n",
    "        super().__init__(action_space=action_space)\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, batch: Batch, state=None, **kwargs) -> Batch:\n",
    "        \"\"\"\n",
    "        Select actions using the model's Q-values.\n",
    "\n",
    "        Implements an epsilon-greedy strategy:\n",
    "            - With probability ε, a random action is chosen.\n",
    "            - Otherwise, the action with the highest Q-value is selected.\n",
    "\n",
    "        Args:\n",
    "            batch (Batch): Batch containing observations.\n",
    "            state: Optional state for recurrent models.\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            Batch: Batch containing selected actions.\n",
    "        \"\"\"\n",
    "        q_values, _ = self.model(batch.obs)\n",
    "        # Greedy action selection\n",
    "        act = q_values.argmax(dim=1).cpu().numpy()\n",
    "        # Epsilon-greedy exploration: random action with probability epsilon\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            act = np.random.randint(0, q_values.shape[1], size=act.shape)\n",
    "        return Batch(act=act)\n",
    "\n",
    "    def learn(self, batch: Batch, next_action: torch.Tensor, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Update the model using TD-learning.\n",
    "\n",
    "        Steps:\n",
    "            1. Compute current Q-values for the actions taken.\n",
    "            2. Estimate the TD target: TD_target = r + γ(1 - d) Q(s', a')\n",
    "            3. Compute the mean squared error (MSE) loss between the current Q-values and the TD target.\n",
    "            4. Backpropagate the loss and update the model parameters using the optimizer.\n",
    "            5. Apply gradient clipping to limit the change in weights.\n",
    "\n",
    "        Args:\n",
    "            batch (Batch): Batch containing observations, actions, rewards, etc.\n",
    "            next_action (torch.Tensor): Next action chosen based on the next observation.\n",
    "            **kwargs: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing the computed loss.\n",
    "        \"\"\"\n",
    "        # Predict Q-values for current states\n",
    "        q_values, _ = self.model(batch.obs)\n",
    "        # Ensure actions are a tensor with correct type and shape\n",
    "        batch_act = (\n",
    "            torch.tensor(batch.act, dtype=torch.long)\n",
    "            if not isinstance(batch.act, torch.Tensor)\n",
    "            else batch.act\n",
    "        )\n",
    "        # Gather the Q-values corresponding to the actions taken\n",
    "        q_values = q_values.gather(1, batch_act.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Compute the TD target without tracking gradients\n",
    "        with torch.no_grad():\n",
    "            next_q_values, _ = self.model(batch.obs_next)\n",
    "            next_q_values = next_q_values.gather(1, next_action.unsqueeze(1)).squeeze(1)\n",
    "            td_target = batch.rew + self.gamma * (1 - batch.done) * next_q_values\n",
    "\n",
    "        # Compute the loss: MSE between current Q-values and TD target\n",
    "        loss = F.mse_loss(q_values, td_target)\n",
    "\n",
    "        # Zero gradients, backpropagate loss, clip gradients, and update parameters\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip gradients to avoid large updates; max_norm can be tuned as needed\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optim.step()\n",
    "\n",
    "        return {\"loss\": loss.item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Q-Network (`QNet`) Full Code\n",
    "\n",
    "The `QNet` class builds a fully connected neural network to predict Q-values. It flattens the state input, passes it through two hidden layers with ReLU activations and layer normalization, and finally outputs the Q-values for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model to predict Q-values from state observations.\n",
    "\n",
    "    Args:\n",
    "        state_shape (tuple): Shape of the input state.\n",
    "        action_shape (int or tuple): Number of actions available.\n",
    "        hidden_size (int, optional): Number of neurons in hidden layers. Default is 128.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "        super().__init__()\n",
    "        # Build the sequential model:\n",
    "        # - First layer: flatten the state and map to hidden_size neurons.\n",
    "        # - ReLU activation introduces non-linearity.\n",
    "        # - LayerNorm stabilizes training by normalizing activations.\n",
    "        # - Second layer: another hidden layer with the same structure.\n",
    "        # - Output layer: maps to the number of actions.\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(np.prod(state_shape), hidden_size),  # Dense layer\n",
    "            nn.ReLU(),                                      # Activation function\n",
    "            nn.LayerNorm(hidden_size),                      # Normalization layer\n",
    "            nn.Linear(hidden_size, hidden_size),            # Hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, np.prod(action_shape))   # Output layer for Q-values\n",
    "        )\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, state=None, info: dict = {}) -> tuple:\n",
    "        \"\"\"\n",
    "        Forward pass to compute Q-values.\n",
    "\n",
    "        Converts input observations to torch tensors (if not already) and passes them\n",
    "        through the network to produce Q-values.\n",
    "\n",
    "        Args:\n",
    "            obs (torch.Tensor or np.ndarray): Input observation.\n",
    "            state: Optional state for recurrent models (unused here).\n",
    "            info (dict): Additional information (unused).\n",
    "\n",
    "        Returns:\n",
    "            tuple: (q_values, state)\n",
    "        \"\"\"\n",
    "        # Convert numpy array to torch tensor if necessary\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32)\n",
    "        q_values = self.net(obs)\n",
    "        return q_values, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create the Environment\n",
    "\n",
    "Let's create the environments!\n",
    "\n",
    "We initialize two environments: one for training and one for testing. The state and action shapes are extracted to build the network correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create training and testing environments using Gymnasium\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "test_env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Extract environment information:\n",
    "# - state_shape: dimensions of the observation space\n",
    "# - action_shape: number of actions\n",
    "state_shape = env.observation_space.shape or (env.observation_space.n,)\n",
    "action_shape = env.action_space.n if hasattr(env.action_space, \"n\") else env.action_space.shape\n",
    "action_space = env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build the Q-Network, Optimizer, and SARSA Policy\n",
    "\n",
    "Here we instantiate the Q-network and set up the optimizer. We then create our custom SARSA policy by passing the network, optimizer, and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Select the appropriate device: CUDA (NVIDIA GPUs), MPS (Apple GPUs), or CPU.\n",
    "# AMD GPUs with ROCm support accessed using the 'cuda' device string.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the Q-network with state and action shapes\n",
    "net = QNet(state_shape, action_shape).to(device)\n",
    "\n",
    "# Set up the Adam optimizer:\n",
    "# - lr: learning rate (can be tuned for faster or slower convergence)\n",
    "# - weight_decay: L2 regularization to prevent overfitting\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=0)\n",
    "\n",
    "# Create the SARSA policy using the network and optimizer.\n",
    "# gamma: discount factor; epsilon: exploration rate for epsilon-greedy strategy.\n",
    "policy = SARSAPolicy(model=net, optim=optimizer, action_space=action_space, gamma=0.995, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Custom Online Training Loop\n",
    "\n",
    "We train the agent using an online approach. This means we collect experience transitions, perform manual conversion of data to torch tensors (if needed), and apply the SARSA learning update. The loop logs rewards and loss for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.data import ReplayBuffer, Collector, Batch\n",
    "import tianshou as ts\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "def kill_port(port):\n",
    "    \"\"\"\n",
    "    Terminates any processes that are listening on the specified port.\n",
    "    Works on both Unix-based systems and Windows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.name == 'nt':\n",
    "            # Windows: Use netstat and taskkill to kill processes on the given port.\n",
    "            # The command below might fail (exit status 1) if no process is found.\n",
    "            cmd = f'for /f \"tokens=5\" %a in (\\'netstat -aon ^| findstr :{port}\\') do taskkill /F /PID %a'\n",
    "            result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Killed processes on port {port}.\")\n",
    "        else:\n",
    "            # Unix (Linux/Mac): Use lsof to find processes on the port and kill them.\n",
    "            cmd = f\"lsof -ti:{port} | xargs kill -9\"\n",
    "            result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Killed processes on port {port}.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # If the error message indicates that no process was found, we can ignore it.\n",
    "        if \"returned non-zero exit status 1\" in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Could not kill process on port {port}: {e}\")\n",
    "\n",
    "# Kill any processes on port 6006 to ensure it is free.\n",
    "kill_port(6006)\n",
    "\n",
    "# Clear previous TensorBoard sessions (cross-platform)\n",
    "tensorboard_info = os.path.join(tempfile.gettempdir(), \".tensorboard-info\")\n",
    "if os.path.exists(tensorboard_info):\n",
    "    shutil.rmtree(tensorboard_info)\n",
    "\n",
    "# Launch TensorBoard in the background on port 6006.\n",
    "tb_command = [\n",
    "    \"tensorboard\",\n",
    "    \"--logdir\", logs_dir,\n",
    "    \"--port\", \"6006\",\n",
    "    \"--host\", \"localhost\",\n",
    "    \"--reload_interval\", \"30\"\n",
    "]\n",
    "tb_process = subprocess.Popen(tb_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Allow time for TensorBoard to start and display its dashboard.\n",
    "time.sleep(5)\n",
    "display(IFrame(src=\"http://localhost:6006\", width=\"100%\", height=\"800\"))\n",
    "\n",
    "#------------------------------------------------------------------------------ \n",
    "# Hyperparameters for training\n",
    "max_epoch = 3               # Total number of epochs for training\n",
    "step_per_epoch = 300        # Steps collected per epoch\n",
    "keep_n_steps = 300          # Number of recent steps to use for learning\n",
    "\n",
    "# Create a ReplayBuffer to store a fixed number of recent transitions.\n",
    "buffer = ReplayBuffer(size=keep_n_steps)\n",
    "\n",
    "# Set up data collectors for training and testing.\n",
    "train_collector = Collector(policy, env, buffer)\n",
    "test_collector = Collector(policy, test_env)\n",
    "\n",
    "# Lists to store epoch summaries.\n",
    "epoch_training_losses = []\n",
    "epoch_test_rewards = []\n",
    "epoch_durations = []\n",
    "\n",
    "global_start_time = time.time()  # Start overall timer\n",
    "\n",
    "# Training loop with progress reporting, logging, and detailed summaries.\n",
    "for epoch in range(max_epoch):\n",
    "    epoch_start_time = time.time()  # Start timer for the epoch\n",
    "    train_collector.reset()\n",
    "    running_loss = 0.0              # Accumulate loss to compute average loss per epoch\n",
    "\n",
    "    # Set up a tqdm progress bar with dynamic post-fix metrics.\n",
    "    progress_bar = tqdm(range(step_per_epoch),\n",
    "                        desc=f\"Epoch {epoch+1}/{max_epoch}\",\n",
    "                        dynamic_ncols=True)\n",
    "    \n",
    "    for step in progress_bar:\n",
    "        # Collect n steps and store transitions in the buffer.\n",
    "        train_collector.collect(n_step=keep_n_steps)\n",
    "        \n",
    "        # Retrieve the last keep_n_steps transitions from the buffer.\n",
    "        batch = train_collector.buffer[-keep_n_steps:]\n",
    "        \n",
    "        # Convert batch fields to torch tensors.\n",
    "        batch.obs = torch.tensor(batch.obs, dtype=torch.float32)\n",
    "        batch.act = torch.tensor(batch.act, dtype=torch.long)\n",
    "        batch.rew = torch.tensor(batch.rew, dtype=torch.float32)\n",
    "        batch.done = torch.tensor(batch.done, dtype=torch.float32)\n",
    "        batch.obs_next = torch.tensor(batch.obs_next, dtype=torch.float32)\n",
    "        \n",
    "        # Determine the next action using the current policy.\n",
    "        next_action = policy.forward(Batch(obs=batch.obs_next)).act\n",
    "        next_action_tensor = torch.tensor(next_action, dtype=torch.long)\n",
    "        \n",
    "        # Perform the SARSA update and capture the loss.\n",
    "        learn_info = policy.learn(batch, next_action_tensor)\n",
    "        loss_val = learn_info.get(\"loss\", 0)\n",
    "        running_loss += loss_val\n",
    "        \n",
    "        global_step = epoch * step_per_epoch + step\n",
    "        \n",
    "        # Log step-level loss continuously to TensorBoard.\n",
    "        logger.writer.add_scalar(\"Loss/train_step\", loss_val, global_step)\n",
    "        logger.writer.add_scalar(\"Loss/train_running_avg\", running_loss / (step + 1), global_step)\n",
    "        \n",
    "        # Optionally flush periodically (or at the end of each epoch).\n",
    "        if step % 50 == 0:\n",
    "            logger.writer.flush()\n",
    "        \n",
    "        # Update progress bar with current metrics.\n",
    "        progress_bar.set_postfix({\n",
    "            \"Step\": f\"{step}/{step_per_epoch}\",\n",
    "            \"Loss\": f\"{loss_val:07.3f}\",\n",
    "            \"AvgLoss\": f\"{running_loss / (step + 1):07.3f}\"\n",
    "        })\n",
    "        \n",
    "        # Print progress summary every 25% of the epoch.\n",
    "        if step % (step_per_epoch // 4) == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}, Step {step}/{step_per_epoch}: \"\n",
    "                f\"Step Loss = {loss_val}, Running Avg Loss = {running_loss/(step+1)}\"\n",
    "            )\n",
    "    \n",
    "    # Compute average loss over the epoch.\n",
    "    avg_loss = running_loss / step_per_epoch\n",
    "\n",
    "    # Reset the test collector and evaluate the agent on 10 episodes.\n",
    "    test_collector.reset()\n",
    "    test_result = test_collector.collect(n_episode=10)\n",
    "    mean_reward = np.mean(test_result[\"rews\"])\n",
    "    std_reward = np.std(test_result[\"rews\"])\n",
    "    min_reward = np.min(test_result[\"rews\"])\n",
    "    p25_reward = np.percentile(test_result[\"rews\"], 25)\n",
    "    median_reward = np.median(test_result[\"rews\"])\n",
    "    p75_reward = np.percentile(test_result[\"rews\"], 75)\n",
    "    max_reward = np.max(test_result[\"rews\"])\n",
    "\n",
    "    # Log epoch-level metrics to TensorBoard.\n",
    "    logger.writer.add_scalar(\"Reward/test_avg\", mean_reward, epoch)\n",
    "    logger.writer.add_scalar(\"Loss/train_avg\", avg_loss, epoch)\n",
    "    logger.writer.flush()\n",
    "\n",
    "    # Calculate epoch elapsed time.\n",
    "    epoch_elapsed = time.time() - epoch_start_time\n",
    "    epoch_training_losses.append(avg_loss)\n",
    "    epoch_test_rewards.append(mean_reward)\n",
    "    epoch_durations.append(epoch_elapsed)\n",
    "\n",
    "    # Print detailed epoch summary.\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch+1} Summary:\\n\"\n",
    "        f\"  - Epoch Elapsed Time      : {epoch_elapsed} seconds\\n\"\n",
    "        f\"  - Steps Collected         : {step_per_epoch}\\n\"\n",
    "        f\"  - Average Training Loss   : {avg_loss}\\n\"\n",
    "        f\"  - Mean Test Reward        : {mean_reward}\\n\"\n",
    "        f\"  - Std Test Reward         : {std_reward}\\n\"\n",
    "        f\"  - Min Test Reward         : {min_reward}\\n\"\n",
    "        f\"  - 25th Percentile Reward  : {p25_reward}\\n\"\n",
    "        f\"  - Median Test Reward      : {median_reward}\\n\"\n",
    "        f\"  - 75th Percentile Reward  : {p75_reward}\\n\"\n",
    "        f\"  - Max Test Reward         : {max_reward}\\n\"\n",
    "    )\n",
    "\n",
    "# Final flush and close the TensorBoard writer.\n",
    "logger.writer.close()\n",
    "\n",
    "# Calculate overall training statistics.\n",
    "total_elapsed = time.time() - global_start_time\n",
    "overall_avg_loss = np.mean(epoch_training_losses)\n",
    "overall_avg_reward = np.mean(epoch_test_rewards)\n",
    "total_epochs = len(epoch_durations)\n",
    "\n",
    "# Print overall summary of all epochs.\n",
    "print(\"\\nOverall Training Summary:\")\n",
    "print(f\"  - Total Epochs            : {total_epochs}\")\n",
    "print(f\"  - Overall Average Loss    : {overall_avg_loss}\")\n",
    "print(f\"  - Overall Average Reward  : {overall_avg_reward}\")\n",
    "print(f\"  - Total Elapsed Time      : {total_elapsed} seconds\")\n",
    "\n",
    "# Reiterate the final epoch's metrics.\n",
    "print(\"\\nFinal Epoch Summary:\")\n",
    "print(\n",
    "    f\"  - Epoch {total_epochs}:\\n\"\n",
    "    f\"      * Average Training Loss : {epoch_training_losses[-1]}\\n\"\n",
    "    f\"      * Average Test Reward   : {epoch_test_rewards[-1]}\\n\"\n",
    "    f\"      * Epoch Elapsed Time    : {epoch_durations[-1]} seconds\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on Key Parameters:**\n",
    "\n",
    "- **Learning Rate (`lr`)** in Adam: A smaller value (e.g., 1e-5) means slower but more stable updates. Increase if learning is too slow.\n",
    "- **Weight Decay:** Regularizes the model by penalizing large weights.\n",
    "- **Gamma (`γ`):** Determines how much future rewards are considered. Values close to 1 favor long-term rewards.\n",
    "- **Epsilon:** Controls exploration. A higher epsilon increases exploration but may delay convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it learn? Do you see rewards increasing? \n",
    "\n",
    "If so let's save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Save the model's state dictionary for future use.\n",
    "model_path = os.path.join(models_dir, f\"{sarsa_td_dir}.pth\")\n",
    "torch.save(net.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Load the Trained Model for Evaluation\n",
    "\n",
    "After saving the model, load it into a new network instance and build a policy for evaluation. Note that we set epsilon to 0 for pure exploitation during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from tianshou.data import Batch  # Ensure Batch is available\n",
    "\n",
    "# Select the appropriate device: CUDA (NVIDIA GPUs), MPS (Apple GPUs), or CPU.\n",
    "# AMD GPUs with ROCm support accessed using the 'cuda' device string.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize a new network with the same architecture and move it to the device.\n",
    "loaded_net = QNet(state_shape, action_shape).to(device)\n",
    "\n",
    "# Load the trained weights, mapping to the appropriate device.\n",
    "model_path = os.path.join(models_dir, f\"{sarsa_td_dir}.pth\")\n",
    "loaded_net.load_state_dict(torch.load(model_path, map_location=device))\n",
    "loaded_net.eval()  # Set the network to evaluation mode\n",
    "\n",
    "# Create a testing environment with rendering enabled.\n",
    "eval_env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Build a new SARSA policy for evaluation using the loaded network.\n",
    "# Set epsilon=0 to disable exploration.\n",
    "loaded_policy = SARSAPolicy(\n",
    "    model=loaded_net,\n",
    "    optim=optimizer,\n",
    "    action_space=action_space,\n",
    "    gamma=0.99,\n",
    "    epsilon=0.0\n",
    ")\n",
    "print(\"Model loaded and evaluation environment initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Run the Agent in the Evaluation Environment\n",
    "\n",
    "Finally, let's test out the model and watch it play the game! The agent will use the loaded model to select actions and interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tianshou.data import Batch  # Ensure Batch is available\n",
    "\n",
    "# Number of episodes to run for evaluation.\n",
    "num_episodes = 20\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = eval_env.reset()\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "\n",
    "    print(f\"Starting episode {episode + 1}\")\n",
    "\n",
    "    while True:\n",
    "        step_count += 1\n",
    "\n",
    "        # Create a Batch for the current observation.\n",
    "        obs_batch = Batch(obs=[obs])\n",
    "        # Disable gradient computations during inference.\n",
    "        with torch.no_grad():\n",
    "            action = loaded_policy.forward(obs_batch).act[0]\n",
    "\n",
    "        # Step the environment with the chosen action.\n",
    "        obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # End the episode if terminated or truncated.\n",
    "        if terminated or truncated:\n",
    "            print(\n",
    "                f\"Episode {episode + 1} ended: Total Reward = {total_reward}, \"\n",
    "                f\"Steps = {step_count}\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(step_count)\n",
    "\n",
    "# Convert lists to numpy arrays for statistical operations.\n",
    "episode_rewards = np.array(episode_rewards)\n",
    "episode_lengths = np.array(episode_lengths)\n",
    "\n",
    "# Compute and print performance statistics if data has been collected.\n",
    "if episode_rewards.size > 0 and episode_lengths.size > 0:\n",
    "    # Calculate statistics for episode rewards.\n",
    "    count_rewards   = len(episode_rewards)\n",
    "    mean_rewards    = np.mean(episode_rewards)\n",
    "    std_rewards     = np.std(episode_rewards)\n",
    "    min_rewards     = np.min(episode_rewards)\n",
    "    p25_rewards     = np.percentile(episode_rewards, 25)\n",
    "    median_rewards  = np.median(episode_rewards)\n",
    "    p75_rewards     = np.percentile(episode_rewards, 75)\n",
    "    max_rewards     = np.max(episode_rewards)\n",
    "    \n",
    "    # Calculate statistics for episode lengths.\n",
    "    count_lengths   = len(episode_lengths)\n",
    "    mean_lengths    = np.mean(episode_lengths)\n",
    "    std_lengths     = np.std(episode_lengths)\n",
    "    min_lengths     = np.min(episode_lengths)\n",
    "    p25_lengths     = np.percentile(episode_lengths, 25)\n",
    "    median_lengths  = np.median(episode_lengths)\n",
    "    p75_lengths     = np.percentile(episode_lengths, 75)\n",
    "    max_lengths     = np.max(episode_lengths)\n",
    "    \n",
    "    # Print the summary table.\n",
    "    print(\"\\nFinal Evaluation Performance Summary:\")\n",
    "    print(f\"Total Episodes Evaluated: {num_episodes}\\n\")\n",
    "    header = \"{:<22} {:>15} {:>20}\".format(\"Statistic\", \"Rewards\", \"Episode Lengths\")\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    print(\"{:<22} {:>15d} {:>20d}\".format(\"Count\", count_rewards, count_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Mean\", mean_rewards, mean_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Std Dev\", std_rewards, std_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Min\", min_rewards, min_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"25th Percentile\", p25_rewards, p25_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Median\", median_rewards, median_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"75th Percentile\", p75_rewards, p75_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Max\", max_rewards, max_lengths))\n",
    "else:\n",
    "    print(\"No performance data was collected. Please verify the Collector configuration.\")\n",
    "\n",
    "# Close the environment once evaluation is complete.\n",
    "eval_env.close()\n",
    "print(\"Evaluation completed and environment closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Things to Try**\n",
    "\n",
    "Experiment with different hyperparameters and settings to see how they affect learning:\n",
    "\n",
    "- **Epsilon:**  \n",
    "  - When to explore?\n",
    "  - *High value:* The agent explores more but the agent cannot take advantage of the environment as they are acting randomly.\n",
    "  - *Low value:* The agent exploits learned behavior but may get stuck in suboptimal actions because the agent gets stuck learning the first useful thing rather than finding the best actions.\n",
    "\n",
    "- **Learning Rate:**  \n",
    "  - How fast to learn new data?\n",
    "  - *High learning rate:* Faster updates may lead to unstable training because the agent learns spurious correlations between actions and outcomes.\n",
    "  - *Low learning rate:* More stable updates but slower learning progress because the agent might take forever to figure out what actions lead to good rewards.\n",
    "\n",
    "- **Discount Factor (γ):**  \n",
    "  - How much does the agent value future vs. near rewards?\n",
    "  - *High gamma:* Emphasizes future rewards, which may slow down learning of immediate actions. \n",
    "  - *Low gamma:* Focuses on short-term rewards and may miss long-term benefits.\n",
    "\n",
    "Adjust these parameters based on the environment and your training goals. Visualize the learning progress with TensorBoard to understand the effects of your changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABE_tutorial_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
