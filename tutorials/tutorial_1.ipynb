{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABE Tutorial 1\n",
    "## Setting up an ABE Workshop\n",
    "\n",
    "In this first tutorial let's setup a workshop to build agents, environments, and RL algorithms!\n",
    "\n",
    "Steps:\n",
    "* Install tianshou\n",
    "* Check that it works\n",
    "* Explore available algorithms and environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tianshou\n",
    "\n",
    "Tianshou is a python library that makes working with deep reinforcement learning easier. It's focus is on developing implimentations of reinforcement learning algorithms that can interact with a wide range of environments. You can read more of the documentation here: https://tianshou.org\n",
    "\n",
    "There are some good tutorials on how to use the different modules of tianshou here: https://tianshou.org/en/stable/02_notebooks/L0_overview.html\n",
    "\n",
    "Below we'll cover most of what's covered in those tutorials here, but with a focus on what we are covering in the ABE book.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Your Data Science Environment with Conda\n",
    "\n",
    "A **virtual environment** is an isolated workspace that lets you maintain separate sets of packages for different projects. This isolation means that changes made in one environment do not affect others, which helps avoid conflicts between package versions.\n",
    "\n",
    "For example, one project might need an older version of a library while another requires the latest version. Virtual environments allow you to work on both projects without interference.\n",
    "\n",
    "You can also export a list of installed packages from a virtual environment to create a reproducible setup. This file can be shared, so others can recreate the same environment for your project.\n",
    "\n",
    "**Conda** is a popular package manager that simplifies creating and managing virtual environments. It lets you install, update, and remove packages, as well as easily switch between different environments.\n",
    "\n",
    "Compared to Python’s default package manager, **pip**, conda includes a package solver that checks for compatibility between dependencies. This feature helps prevent conflicts and makes it simpler to install complex libraries that have many dependencies, including non-Python ones (for instance, libraries in C or C++). For example, deep learning frameworks like TensorFlow and PyTorch, which often rely on system-level libraries such as CUDA for GPU support, are easier to install and update with conda.\n",
    "\n",
    "**Miniforge** offers a minimal installer for conda that is pre-configured to use the community-driven **conda-forge** channel and uses the **libmamba** solver by default for faster dependency resolution.\n",
    "\n",
    "### Installing Conda using Miniforge\n",
    "\n",
    "If you would like to download Miniforge and install Conda manually using a GUI, visit the [Miniforge releases page](https://github.com/conda-forge/miniforge/releases/latest) and download the appropriate installer for your system.\n",
    "\n",
    "**Otherwise**, below are streamlined instructions to download Miniforge and install Conda using command-line tools for Windows, macOS, and Linux:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Windows\n",
    "\n",
    "1. **Open Windows PowerShell as Administrator**:\n",
    "   - Press `Win + X` and select **Windows PowerShell (Admin)**.\n",
    "   - Or, search for `PowerShell`, right-click the result, and select **Run as administrator**.\n",
    "\n",
    "2. **Download the Miniforge Installer**:\n",
    "   - Run the following command to download the latest Miniforge installer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Download the Miniforge installer\n",
    "Invoke-WebRequest -Uri \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Windows-x86_64.exe\" -OutFile \"$env:TEMP\\Miniforge3-Windows-x86_64.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Install Conda Silently**:\n",
    "   - Execute the installer with default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Install Miniforge silently to the user folder\n",
    "Start-Process -FilePath \"$env:TEMP\\Miniforge3-Windows-x86_64.exe\" -ArgumentList \"/InstallationType=JustMe /RegisterPython=0 /S /D=$env:USERPROFILE\\Miniforge3\" -NoNewWindow -Wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Add Miniforge to the System PATH and Initialize Conda**:\n",
    "   - After running the commands below, close and reopen PowerShell to apply the changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Permanently add Miniforge directories to the user PATH\n",
    "$currentUserPath = [Environment]::GetEnvironmentVariable(\"Path\", \"User\")\n",
    "$newEntries = \"$env:USERPROFILE\\Miniforge3\\Scripts;$env:USERPROFILE\\Miniforge3\\condabin\"\n",
    "if ($currentUserPath -notlike \"*$env:USERPROFILE\\Miniforge3*\") {\n",
    "    $newUserPath = \"$currentUserPath;$newEntries\"\n",
    "    [Environment]::SetEnvironmentVariable(\"Path\", $newUserPath, \"User\")\n",
    "}\n",
    "\n",
    "# Initialize Conda for PowerShell\n",
    "conda init powershell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Remove Any Conflicting Alias for Conda**:\n",
    "   - Open your AllHosts profile in Notepad, by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "notepad $PROFILE.CurrentUserAllHosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - At the end of the file, add the following line, then save and close the file, and restart PowerShell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "Remove-Item alias:conda -ErrorAction SilentlyContinue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### macOS\n",
    "\n",
    "1. **Open Terminal**:\n",
    "   - Navigate to **Applications** > **Utilities** > **Terminal**.\n",
    "\n",
    "2. **Download the Miniforge Installer**:\n",
    "   - Use `curl` to download the installer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Download the Miniforge installer\n",
    "curl -fsSLo Miniforge3-MacOSX-$(uname -m).sh \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-$(uname -m).sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Run the Installer**:\n",
    "   - Make the installer executable and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Make the installer executable\n",
    "chmod +x Miniforge3-MacOSX-$(uname -m).sh\n",
    "\n",
    "# Install Miniforge silently (batch mode)\n",
    "./Miniforge3-MacOSX-$(uname -m).sh -b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Permanently Add Miniforge to the System PATH**:\n",
    "   - Depending on which shell you use, run one of the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Permanently add Miniforge to PATH\n",
    "\n",
    "# If using bash:\n",
    "echo 'export PATH=\"$HOME/miniforge3/bin:$PATH\"' >> ~/.bash_profile\n",
    "\n",
    "# If using zsh:\n",
    "echo 'export PATH=\"$HOME/miniforge3/bin:$PATH\"' >> ~/.zshrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Initialize Conda**:\n",
    "   - Run the following command to initialize Conda, then close and reopen Terminal to apply the changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Conda for the current shell\n",
    "~/miniforge3/bin/conda init \"$(basename \"${SHELL}\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linux\n",
    "\n",
    "1. **Open Terminal**.\n",
    "\n",
    "2. **Download the Miniforge Installer**:\n",
    "   - Use `wget` to download the installer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Download the Miniforge installer\n",
    "wget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-$(uname -m).sh\" -O Miniforge3-Linux.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Run the Installer**:\n",
    "   - Make the installer executable and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Make the installer executable\n",
    "chmod +x Miniforge3-Linux.sh\n",
    "\n",
    "# Install Miniforge silently (batch mode)\n",
    "./Miniforge3-Linux.sh -b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Permanently Add Miniforge to the System PATH**:\n",
    "   - Depending on which shell you use, run one of the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Permanently add Miniforge to PATH\n",
    "\n",
    "# If using bash:\n",
    "echo 'export PATH=\"$HOME/miniforge3/bin:$PATH\"' >> ~/.bash_profile\n",
    "\n",
    "# If using zsh:\n",
    "echo 'export PATH=\"$HOME/miniforge3/bin:$PATH\"' >> ~/.zshrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Initialize Conda**:\n",
    "   - Run the following command to initialize Conda, then close and reopen Terminal to apply the changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Conda for the current shell\n",
    "~/miniforge3/bin/conda init \"$(basename \"${SHELL}\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Conda Environments\n",
    "\n",
    "#### Update Conda\n",
    "\n",
    "Before creating a new environment, verify that conda is up to date by first updating conda, then updating the base environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda update conda\n",
    "conda update --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a New Environment\n",
    "\n",
    "To create a new environment named `ABE_tutorial_env` with a specific version of Python (3.12 in this case), use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda create --name ABE_tutorial_env python=3.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activate an Environment\n",
    "\n",
    "To activate the environment, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda activate ABE_tutorial_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Commands\n",
    "\n",
    "- Deactivate an Environment: `conda deactivate`\n",
    "- List all Environments: `conda env list`\n",
    "- Clone an Environment: `conda create --name new_env --clone ABE_tutorial_env`\n",
    "- Remove an Environment: `conda env remove --name ABE_tutorial_env`\n",
    "- Export Environment to a File: `conda env export --name ABE_tutorial_env > environment.yml`\n",
    "- Create Environment from File: `conda env create --file environment.yml`\n",
    "\n",
    "#### Additional Resources\n",
    "\n",
    "For more detailed information on conda, consider the following resources:\n",
    "\n",
    "- **Official Documentation**: https://docs.conda.io/projects/conda/en/latest/\n",
    "\n",
    "- **Cheat Sheet**: https://docs.conda.io/projects/conda/en/latest/user-guide/cheatsheet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installing Required Packages\n",
    "\n",
    "To install the required packages for this series of tutorials, first ensure that you have activated the `ABE_tutorial_env` environment. Then, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda install pytorch tianshou gymnasium mujoco cudatoolkit numpy tensorboard torchinfo matplotlib torchvision imageio imageio-ffmpeg pygame ipython ipykernel ipywidgets tqdm torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Package**       | **Description**                                                                                                                                                          | **Deep Reinforcement Learning Use Examples**                                                                                                                                                   |\n",
    "|-------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **PyTorch**       | A deep learning library for efficient tensor computations and dynamic network construction.                                                                             | Constructing and training neural networks for policies and value functions in deep RL models.                                                                                                   |\n",
    "| **Tianshou**      | A reinforcement learning library built on PyTorch that offers flexible algorithm implementations and training utilities.                                                 | Implementing RL algorithms such as DQN, PPO, and A2C using customizable training loops.                                                                                                        |\n",
    "| **Gymnasium**     | A toolkit providing standardized environments and interfaces for developing and evaluating RL algorithms.                                                                  | Simulating diverse environments—from classic control tasks to Atari games—for training and benchmarking RL agents.                                                                                |\n",
    "| **Mujoco**        | A high-fidelity physics engine designed for simulating continuous control tasks, particularly in robotics.                                                                 | Training agents in realistic continuous control scenarios like robotic locomotion and manipulation.                                                                                             |\n",
    "| **CUDA Toolkit**  | A suite of tools for GPU acceleration that speeds up tensor computations and model training.                                                                              | Accelerating the training process of deep RL models, especially when handling large networks or complex simulation environments.                                                                |\n",
    "| **NumPy**         | A numerical computing library that supports multi-dimensional arrays and matrices.                                                                                        | Managing state representations and performing batch computations required by RL algorithms.                                                                                                     |\n",
    "| **TensorBoard**   | A visualization tool for tracking training metrics and model architectures during deep learning experiments.                                                               | Monitoring reward curves, loss trends, and network structures throughout deep RL training sessions.                                                                                            |\n",
    "| **TorchInfo**     | A model inspection tool that provides detailed summaries of PyTorch neural network architectures, including layer outputs and parameter counts.                         | Debugging and verifying network architectures used in deep RL to ensure proper layer configurations and resource allocation.                                                                   |\n",
    "| **Matplotlib**    | A plotting library for creating static, animated, and interactive visualizations.                                                                                        | Visualizing training progress and performance metrics of deep RL agents over time.                                                                                                                |\n",
    "| **TorchVision**   | A library offering datasets, pre-trained models, and image transformation utilities focused on computer vision tasks.                                                      | Preprocessing and augmenting visual inputs—such as frames from game environments—for image-based RL tasks.                                                                                        |\n",
    "| **Imageio**       | A library for reading and writing images in various formats, useful for managing simulation frames.                                                                       | Saving and processing individual image frames produced during RL experiments for further analysis or reporting.                                                                                 |\n",
    "| **Imageio-ffmpeg**| A Python wrapper for FFMPEG that provides video encoding and decoding capabilities.                                                                                      | Recording simulation frames and converting them into video files to visually inspect agent performance during training.                                                                          |\n",
    "| **Pygame**        | A set of modules for creating games and interactive graphical interfaces in Python.                                                                                      | Building custom RL environments or rendering real-time visualizations of agent behavior in simulation settings.                                                                                 |\n",
    "| **IPython**       | An interactive computing environment offering an enhanced Python shell for prototyping and quick experimentation.                                                         | Prototyping RL code interactively and debugging algorithms step by step before deploying full-scale experiments.                                                                                 |\n",
    "| **ipykernel**     | The IPython kernel that powers Jupyter notebooks, enabling interactive execution of Python code.                                                                          | Supporting dynamic RL experiments in notebook environments, which are ideal for iterative development and testing.                                                                               |\n",
    "| **ipywidgets**    | A collection of interactive HTML widgets that facilitate the creation of dynamic user interfaces in Jupyter notebooks.                                                     | Creating interactive dashboards and controls (like sliders) for real-time hyperparameter tuning and monitoring during RL training.                                                                 |\n",
    "| **tqdm**          | A fast, extensible progress bar library that provides visual feedback for long-running loops.                                                                              | Tracking the progress of training episodes or iterations in deep RL experiments, offering immediate insight into execution status.                                                              |\n",
    "| **TorchAudio**    | A library for loading, transforming, and analyzing audio data.                                                                                                           | Processing auditory signals when RL agents incorporate sound cues or require audio-based feedback as part of their input state.                                                                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command installs:\n",
    "\n",
    "- **torchsummary**: Provides a summary of PyTorch model architectures.\n",
    "\n",
    "Using `pip` within a conda environment is common, but it's important to remember that `pip` and `conda` manage packages differently. To avoid conflicts, it's best to use `conda` whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Installed Packages\n",
    "\n",
    "#### List Installed Packages\n",
    "\n",
    "To verify that the packages are installed correctly, activate the `ABE_tutorial_env` environment. Then, you can list the installed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command displays all packages installed in the current environment.\n",
    "\n",
    "#### Check PyTorch and Tianshou Installation\n",
    "\n",
    "Make sure that this notebook is running in the `ABE_tutorial_env` environment. Then, run the following code to check the PyTorch installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If PyTorch is installed correctly, you should see the version number and whether PyTorch has access to your GPU.\n",
    "\n",
    "To check the Tianshou installation, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tianshou\n",
    "print(tianshou.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Tianshou is installed correctly, you should see the version number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train our first RL agent\n",
    "\n",
    "For this first example we'll focus just on the pieces that need to be in place for us to train an RL agent. Later as we move through the tutorials we'll learn more about each of the pieces and even start to customize some of them!\n",
    "\n",
    "But for now let's use existing RL algorithms and some existing environments to just see how it all works together.\n",
    "\n",
    "Import some libraries:\n",
    "\n",
    "* **gymnasium** will have some environments for us to use (https://gymnasium.farama.org/)\n",
    "* **torch** will let us build some neural networks (https://pytorch.org/)\n",
    "* **TensorBoard** will let us see how well our agent is doing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a unique timestamped ID for our agent so that we can keep track of it in TensorBoard and compare to other agents later. Then we create subdirectories for the logs and models that we'll save during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Timestamped ID for this run to avoid overwriting previous runs and to keep track of different runs\n",
    "agent_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Format: YYYYMMDD_HHMMSS\n",
    "\n",
    "first_dqn_dir = os.path.join(\"dqn\", agent_id)\n",
    "os.makedirs(first_dqn_dir, exist_ok=True) # Ensure the directory exists\n",
    "print(f\"All files for this run will be saved in the directory: {first_dqn_dir}\")\n",
    "\n",
    "logs_dir = os.path.join(first_dqn_dir, \"logs\")\n",
    "os.makedirs(logs_dir, exist_ok=True) # Ensure the directory exists\n",
    "print(f\"Tensorboard logs will be saved in the directory: {logs_dir}\")\n",
    "\n",
    "models_dir = os.path.join(first_dqn_dir, \"models\")\n",
    "os.makedirs(models_dir, exist_ok=True) # Ensure the directory exists\n",
    "print(f\"Models will be saved in the directory: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then start a \"logger\" so we can see what is going on. The code below will create a directory to store the logs of our agent. We will use this to store the summary statistics of our agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = ts.utils.TensorboardLogger(SummaryWriter(logs_dir))\n",
    "print(f\"TensorBoard logs are being saved in: {logs_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup an environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's create an instance of the CartPole environment from gymnasium. This is a simple environment where the agent must balance a pole on a cart by moving the cart left or right.\n",
    "\n",
    "In this environment, the agent receives a reward of +1 for each time step the pole remains upright. The episode ends if either:\n",
    "\n",
    "- the pole falls over (angle > 12 degrees from vertical)\n",
    "- the cart moves too far to the left or right (position > 2.4 units from center)\n",
    "- the episode length is greater than 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CartPole_env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the agent able to see in this environment?**\n",
    "\n",
    "To find out, we can check the observation space of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CartPole_env.observation_space)\n",
    "print(type(CartPole_env.observation_space))\n",
    "print(CartPole_env.observation_space.dtype)\n",
    "print(CartPole_env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the observation space is a Box space with four dimensions. This means that the agent receives a four-dimensional observation at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CartPole_env.observation_space.low)\n",
    "print(CartPole_env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us the lower and upper bounds of the observation space, which means that the agent's observations will be within these bounds.\n",
    "\n",
    "Each of the four dimensions corresponds to a different aspect of the environment:\n",
    "\n",
    "1. The cart's horizontal position (from -4.8 to 4.8)\n",
    "2. The cart's velocity (from -Inf to Inf)\n",
    "3. The pole's angle (from ~ -0.418 rad (-24°) to ~ 0.418 rad (24°))\n",
    "4. The pole's angular velocity (from -Inf to Inf)\n",
    "\n",
    "When the environment is reset at the beginning of each episode, each of these four values is initialized to a random value between -0.05 and 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also ask: **What actions can the agent take in this environment?**\n",
    "\n",
    "To find out, we can check the action space of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CartPole_env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn that the agent can take one of two discrete actions at each time step:\n",
    "\n",
    "- Action 0: Move the cart to the left\n",
    "- Action 1: Move the cart to the right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building our agent. \n",
    "\n",
    "To start off let's build a neural network that take what the agent observes and converts that into actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the network class and utility function for flattening spaces.\n",
    "from tianshou.utils.net.common import Net\n",
    "from gymnasium.spaces.utils import flatdim\n",
    "\n",
    "# Select the appropriate device: CUDA (NVIDIA GPUs), MPS (Apple GPUs), or CPU.\n",
    "# AMD GPUs with ROCm support accessed using the 'cuda' device string.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get observation and action dimensions from the CartPole environment.\n",
    "state_shape = flatdim(CartPole_env.observation_space)   # Total number of elements in the observation space.\n",
    "action_shape = flatdim(CartPole_env.action_space)       # Total number of elements in the action space (usually the number of discrete actions).\n",
    "\n",
    "# Build a network that maps observations to action values.\n",
    "# Available parameters:\n",
    "#   - state_shape: Dimension of the flattened observation.\n",
    "#   - action_shape: Dimension of the action output (number of actions).\n",
    "#   - hidden_sizes: List defining the sizes of hidden layers (adjust for model capacity).\n",
    "#   - device (optional): Computation device ('cpu' or 'cuda').\n",
    "#   - activation (optional): Activation function between layers (default: torch.nn.ReLU).\n",
    "\n",
    "net = Net(\n",
    "    state_shape=state_shape,        # Input dimension (flattened observation).\n",
    "    action_shape=action_shape,      # Output dimension (number of actions).\n",
    "    hidden_sizes=[128, 128, 128],   # Hidden layer sizes; change to adjust model complexity.\n",
    "    device=device,                  # Device for computations; switch to 'cuda' if using a GPU.\n",
    "    # activation=torch.nn.ReLU      # Activation function; consider torch.nn.Tanh for different behavior.\n",
    ")\n",
    "\n",
    "# Print the network architecture.\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden_sizes argument above is for setting the number of nodes within each neural network layer that link the initial input (i.e., observations of the current state: 4) and the possible actions (i.e., 2 discrete actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need to build an optimizer to allow our agent to learn! This optimizer will adjust the weights in the neural network to link observations to actions that lead to more rewards.\n",
    "\n",
    "In the case of the carte pole environment the rewards are the steps where the pole is held upright (i.e., less that 12 degrees from verticle).\n",
    "\n",
    "We'll use a pre-built optimizer called Adam, with a learning rate of 0.001. The learning rate determines how quickly the agent adapts it's weights during each step. This will be a hyperparameter that we will use more later in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Adam optimizer to update the network parameters.\n",
    "\n",
    "# Available parameters for torch.optim.Adam:\n",
    "#   - params: Iterable of parameters to optimize.\n",
    "#   - lr: Learning rate (default: 0.001 here).\n",
    "#   - eps (optional): Term added for numerical stability (default: 1e-08).\n",
    "#   - weight_decay (optional): L2 penalty (default: 0).\n",
    "#   - amsgrad (optional): Boolean flag to enable AMSGrad variant (default: False).\n",
    "\n",
    "optim = torch.optim.Adam(\n",
    "    net.parameters(),       # Parameters of the network to optimize.\n",
    "    lr=0.001                # Learning rate; adjust to speed up or slow down convergence.\n",
    "    # eps=1e-08,            # Small constant for numerical stability.\n",
    "    # weight_decay=0,       # L2 regularization factor to prevent overfitting.\n",
    "    # amsgrad=False         # Whether to use the AMSGrad variant of Adam.\n",
    ")\n",
    "\n",
    "# Print the optimizer details.\n",
    "print(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a network and an optimizer let's define a policy that will control how learning takes place.\n",
    "\n",
    "> The discount factor is how much the agent takes into acount future rewards vs. immediate rewards. A choice of 0.9 suggest that the agent should prioritize future rewards, while a choice of 0.1 suggests the agent should prioritize immediate rewards.\n",
    "\n",
    "> estimation_step is how many steps into the future the agent should look when calculating the value of different actions.\n",
    "\n",
    "> target_update_freq is how many steps should be taken before updating the network weights to match what the agent is learning.\n",
    "\n",
    "Some of these parameters are specific to the RL algorithm we are using here (i.e., estimation_step, and target_update_freq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DQN policy.\n",
    "\n",
    "# Available parameters for ts.policy.DQNPolicy:\n",
    "#   - model: The Q-network approximating state-action values.\n",
    "#   - optim: Optimizer for the network parameters.\n",
    "#   - discount_factor: Gamma; balances immediate vs. future rewards.\n",
    "#   - action_space: Informs the policy of available actions.\n",
    "#   - estimation_step: Number of steps for n-step return calculations.\n",
    "#   - target_update_freq: Steps between updating the target network (ensures stability).\n",
    "#   - reward_normalization (optional): Flag to normalize rewards.\n",
    "\n",
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net,                                  # Q-network approximating state-action values.\n",
    "    optim=optim,                                # Network optimizer; adjust learning rate or try a different optimizer if needed.\n",
    "    discount_factor=0.9,                        # Gamma; values near 1 favor long-term rewards.\n",
    "    action_space=CartPole_env.action_space,     # Provides the policy with information about available actions.\n",
    "    estimation_step=3,                          # Number of steps in n-step returns; higher values may incorporate more future rewards.\n",
    "    target_update_freq=320,                     # Frequency (in steps) to update the target network; higher values yield more stable targets.\n",
    "    # reward_normalization=False,               # Whether to normalize rewards; set True if rewards have high variance.\n",
    ")\n",
    "\n",
    "# Print the policy details.\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's setup a collector to feed observations to the policy as the agent interacts with it's environment.\n",
    "\n",
    "> We'll add a test collector that will run tests periodically to see how well our agent is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training and testing data collectors.\n",
    "\n",
    "# Available parameters for ts.data.Collector:\n",
    "#   - policy: The policy used to interact with the environment.\n",
    "#   - env: The environment from which to collect experiences.\n",
    "#   - buffer: Memory buffer for storing experiences. Here, VectorReplayBuffer is used for vectorized environments.\n",
    "#   - exploration_noise (optional): Boolean to add extra noise during action selection; typically not used for DQN.\n",
    "#   - preprocess_fn (optional): Function to preprocess data before storing.\n",
    "\n",
    "train_collector = ts.data.Collector(\n",
    "    policy,                                 # Policy used to collect training experiences.\n",
    "    CartPole_env,                           # Environment instance for training.\n",
    "    ts.data.VectorReplayBuffer(10000, 1),   # Replay buffer with capacity 10,000 and 1 environment; adjust capacity if needed.\n",
    "    exploration_noise=True                  # Enables additional exploration noise; note that DQN typically uses epsilon-greedy.\n",
    ")\n",
    "\n",
    "# Setup the test collector for evaluation.\n",
    "# For testing, exploration noise is usually enabled/disabled based on the evaluation strategy.\n",
    "test_collector = ts.data.Collector(\n",
    "    policy,                         # Policy used for evaluation.\n",
    "    CartPole_env,                   # Environment instance for testing.\n",
    "    exploration_noise=True          # Exploration noise; often kept lower to assess the deterministic performance.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have:\n",
    "\n",
    "1. An environment\n",
    "2. A Policy with a network model and an optimizer\n",
    "3. A collector to store the agent experiences\n",
    "\n",
    "We can now train our agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use something called an Off Policy Trainer for now. This trainer controls the learning of a main offline neural network model, and only periodically updates a second version of this neural network that is used by the agent to make descisions. This helps with the stability of the training/learning. However, we'll see in a few tutorials how we can have a fully online trainer where there is no distinction between an off line neural network model and the one being used by the agent to learn. \n",
    "\n",
    "Parameters:\n",
    "> max_epochs is the number of rounds of training to run before stopping the training.\n",
    "\n",
    "> steps_per_epoch is the number actions the agent will take per epoch\n",
    "\n",
    "> steps_per_collect is the number of actions to take before collecting experiences in the replay buffer (a list of stored experiences)\n",
    "\n",
    "> episode_per_test is the number of episodes to run during testing that occurs at the end of an epoch. This estimates how much our agent has learned.\n",
    "\n",
    "> batch_size is the amount of experiences to take from the replay buffer when trianing the neural network model.\n",
    "\n",
    "> train_fn is a function that is called at the start of each training epoch. Here it sets the eps (epsilon) paramter to 0.1. Telling the agent to try an exploritory action 10% of the time, rather than what the agent thinks is the best current action.\n",
    "\n",
    "> test_fn is the same as train_fn, just with the test environment.\n",
    "\n",
    "> stop_fn is a function that will stop the training if its conditions are met. Here is stops when the rewards reach a specific threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.data import ReplayBuffer, Collector, Batch\n",
    "import tianshou as ts\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "def kill_port(port):\n",
    "    \"\"\"\n",
    "    Terminates any processes that are listening on the specified port.\n",
    "    Works on both Unix-based systems and Windows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.name == 'nt':\n",
    "            # Windows: Use netstat and taskkill to kill processes on the given port.\n",
    "            # The command below might fail (exit status 1) if no process is found.\n",
    "            cmd = f'for /f \"tokens=5\" %a in (\\'netstat -aon ^| findstr :{port}\\') do taskkill /F /PID %a'\n",
    "            result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Killed processes on port {port}.\")\n",
    "        else:\n",
    "            # Unix (Linux/Mac): Use lsof to find processes on the port and kill them.\n",
    "            cmd = f\"lsof -ti:{port} | xargs kill -9\"\n",
    "            result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Killed processes on port {port}.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # If the error message indicates that no process was found, we can ignore it.\n",
    "        if \"returned non-zero exit status 1\" in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Could not kill process on port {port}: {e}\")\n",
    "\n",
    "# Kill any processes on port 6006 to ensure it is free.\n",
    "kill_port(6006)\n",
    "\n",
    "# Clear previous TensorBoard sessions (cross-platform)\n",
    "tensorboard_info = os.path.join(tempfile.gettempdir(), \".tensorboard-info\")\n",
    "if os.path.exists(tensorboard_info):\n",
    "    shutil.rmtree(tensorboard_info)\n",
    "\n",
    "# Launch TensorBoard in the background on port 6006.\n",
    "tb_command = [\n",
    "    \"tensorboard\",\n",
    "    \"--logdir\", logs_dir,\n",
    "    \"--port\", \"6006\",\n",
    "    \"--host\", \"localhost\",\n",
    "    \"--reload_interval\", \"30\"\n",
    "]\n",
    "tb_process = subprocess.Popen(tb_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Allow time for TensorBoard to start and display its dashboard.\n",
    "time.sleep(5)\n",
    "display(IFrame(src=\"http://localhost:6006\", width=\"100%\", height=\"800\"))\n",
    "\n",
    "#------------------------------------------------------------------------------ \n",
    "\n",
    "# Start training using OffpolicyTrainer.\n",
    "\n",
    "# Available parameters for ts.trainer.OffpolicyTrainer:\n",
    "#   - policy: The policy instance to be trained.\n",
    "#   - train_collector: Collector that gathers training experiences.\n",
    "#   - test_collector: Collector that gathers evaluation data.\n",
    "#   - max_epoch: Maximum number of epochs for training.\n",
    "#   - step_per_epoch: Number of environment steps per epoch.\n",
    "#   - step_per_collect: Steps to collect between each policy update.\n",
    "#   - episode_per_test: Number of episodes to run during each test phase.\n",
    "#   - batch_size: Mini-batch size for sampling from the replay buffer.\n",
    "#   - update_per_step: Ratio of gradient updates per collected environment step.\n",
    "#   - train_fn: Function executed during training (e.g., to adjust epsilon for exploration).\n",
    "#   - test_fn: Function executed during testing (e.g., to adjust epsilon).\n",
    "#   - stop_fn: Function to decide when to stop training (e.g., when reward threshold is met).\n",
    "#   - logger: Logger to capture training progress and metrics.\n",
    "#   - save_checkpoint_fn: Function to save training checkpoints (default: None).\n",
    "\n",
    "trainer = ts.trainer.OffpolicyTrainer(\n",
    "    policy=policy,                                                                      # DQN policy to be trained.\n",
    "    train_collector=train_collector,                                                    # Collector for training experiences.\n",
    "    test_collector=test_collector,                                                      # Collector for evaluation data.\n",
    "    max_epoch=5,                                                                        # Total training epochs; increase for longer training.\n",
    "    step_per_epoch=10000,                                                               # Number of environment steps per epoch.\n",
    "    step_per_collect=100,                                                               # Steps to collect between each update; lower values mean more frequent updates.\n",
    "    episode_per_test=10,                                                                # Episodes to run during evaluation; adjust for reliable performance metrics.\n",
    "    batch_size=64,                                                                      # Mini-batch size for training updates; modify based on memory and stability.\n",
    "    update_per_step=1 / 10,                                                             # Ratio of gradient updates per collected environment step.\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(0.1),                               # Function to adjust training parameters; here setting epsilon to 0.1.\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(0.05),                               # Function to adjust evaluation parameters; here lowering epsilon to 0.05.\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= CartPole_env.spec.reward_threshold,    # Stops training when average reward meets/exceeds the environment's threshold.\n",
    "    logger=logger,                                                                      # Logger for tracking and recording training progress.\n",
    "    # save_checkpoint_fn=None                                                           # Optional function to save training checkpoints.\n",
    ").run()\n",
    "\n",
    "# Print the full training summary.\n",
    "print(\"\\nTraining Summary:\\n\")\n",
    "for key, value in trainer.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the code has finished you can save the trained agent to be used later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Save the model's state dictionary for future use.\n",
    "model_path = os.path.join(models_dir, f\"{first_dqn_dir}.pth\")\n",
    "torch.save(net.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then load the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Select the appropriate device: CUDA (NVIDIA GPUs), MPS (Apple GPUs), or CPU.\n",
    "# AMD GPUs with ROCm support accessed using the 'cuda' device string.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "                      \"mps\" if torch.backends.mps.is_available() else\n",
    "                      \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the trained policy\n",
    "model_path = os.path.join(models_dir, f\"{first_dqn_dir}.pth\")\n",
    "policy.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works by watching the trained agent in a new environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment with rendering enabled.\n",
    "# The 'render_mode' set to \"human\" allows you to visually inspect the agent's performance.\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment to start with an initial state.\n",
    "# The returned state is not used further, but the call ensures the environment is ready.\n",
    "env.reset()\n",
    "\n",
    "# Set the policy to evaluation mode.\n",
    "# This ensures that layers like dropout or batch normalization work in inference mode.\n",
    "policy.eval()\n",
    "\n",
    "# Create a Collector that manages the interaction between the policy and the environment.\n",
    "# For evaluation, we disable extra exploration by setting 'exploration_noise' to False.\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=False)\n",
    "\n",
    "# Testing parameters:\n",
    "n_episodes = 20         # Total number of episodes to run for testing.\n",
    "frame_rate = 1 / 60     # Render delay between frames (60 frames per second).\n",
    "\n",
    "# Collect data for the specified number of episodes.\n",
    "# The 'render' parameter controls the visualization speed.\n",
    "results = collector.collect(n_episode=n_episodes, render=frame_rate)\n",
    "\n",
    "# Extract performance data from the results.\n",
    "# 'rews' is expected to be a list or array of total rewards per episode.\n",
    "# 'lens' is expected to be a list or array of the episode lengths (number of steps).\n",
    "episode_rewards = results.get(\"rews\", [])\n",
    "episode_lengths = results.get(\"lens\", [])\n",
    "\n",
    "# Close the environment to free up resources and close any open rendering windows.\n",
    "env.close()\n",
    "\n",
    "# Convert lists to numpy arrays for statistical operations.\n",
    "episode_rewards = np.array(episode_rewards)\n",
    "episode_lengths = np.array(episode_lengths)\n",
    "\n",
    "# Compute and print performance statistics if data has been collected.\n",
    "if episode_rewards.size > 0 and episode_lengths.size > 0:\n",
    "    # Calculate statistics for episode rewards.\n",
    "    count_rewards   = len(episode_rewards)\n",
    "    mean_rewards    = np.mean(episode_rewards)\n",
    "    std_rewards     = np.std(episode_rewards)\n",
    "    min_rewards     = np.min(episode_rewards)\n",
    "    p25_rewards     = np.percentile(episode_rewards, 25)\n",
    "    median_rewards  = np.median(episode_rewards)\n",
    "    p75_rewards     = np.percentile(episode_rewards, 75)\n",
    "    max_rewards     = np.max(episode_rewards)\n",
    "    \n",
    "    # Calculate statistics for episode lengths.\n",
    "    count_lengths   = len(episode_lengths)\n",
    "    mean_lengths    = np.mean(episode_lengths)\n",
    "    std_lengths     = np.std(episode_lengths)\n",
    "    min_lengths     = np.min(episode_lengths)\n",
    "    p25_lengths     = np.percentile(episode_lengths, 25)\n",
    "    median_lengths  = np.median(episode_lengths)\n",
    "    p75_lengths     = np.percentile(episode_lengths, 75)\n",
    "    max_lengths     = np.max(episode_lengths)\n",
    "    \n",
    "    # Print the summary table.\n",
    "    print(\"Final Evaluation Performance Summary:\")\n",
    "    print(f\"Total Episodes Evaluated: {n_episodes}\\n\")\n",
    "    header = \"{:<22} {:>15} {:>20}\".format(\"Statistic\", \"Rewards\", \"Episode Lengths\")\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    print(\"{:<22} {:>15d} {:>20d}\".format(\"Count\", count_rewards, count_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Mean\", mean_rewards, mean_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Std Dev\", std_rewards, std_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Min\", min_rewards, min_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"25th Percentile\", p25_rewards, p25_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Median\", median_rewards, median_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"75th Percentile\", p75_rewards, p75_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Max\", max_rewards, max_lengths))\n",
    "else:\n",
    "    print(\"No performance data was collected. Please verify the Collector configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does your trained agent do? Was it better than you at keeping the pole upright?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to try**\n",
    "\n",
    "> Changing the environment to another classic control environment\n",
    "\n",
    ">> Go to https://gymnasium.farama.org/environments/classic_control/\n",
    "\n",
    ">> Choose another environment (it has to have discrete actions with the RL algorithm we are using here!): mountain car or acrobot. We'll learn in more depth other algorithms that can do both discrete and continuous actions.\n",
    "\n",
    ">> Use the code above as a guide and attempt to run an agent on one of these new environments below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try out another environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "From this tutorial you should be able to now train an agent in some discrete environments using tianshou. We'll start to dig deeper into each of the sections above, to get a better sense of how each part works, and to make the training closer to the contiuous learning within one-life time that is closer to the challenges faced by animals learning to behave in their environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABE_tutorial_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
