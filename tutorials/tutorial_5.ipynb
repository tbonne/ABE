{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABE Tutorial 5  \n",
    "## Using Continuous Actions\n",
    "\n",
    "In this tutorial, we extend our previous Actor-Critic (A2C) implementation to handle continuous action spaces. Instead of selecting from a set of discrete actions (e.g., left or right), our agent will choose from a continuous range of actions (e.g., a movement of $-0.23$ or $+0.12$). This flexibility allows us to model more realistic and complex environments.\n",
    "\n",
    "**Tutorial Outline:**\n",
    "- Converting A2C to work with continuous action spaces\n",
    "- Testing the continuous A2C algorithm in new environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Walkthrough**: Continuous A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous tutorials, the agent’s actions were limited to a set of discrete choices. In many real-world scenarios (for example, controlling a robot or a vehicle), it is more natural to work with continuous actions.\n",
    "\n",
    "Here, we explain how to modify the actor network and policy so that:\n",
    "- The actor outputs parameters (mean and standard deviation) of a Gaussian distribution.\n",
    "- Actions are sampled from this distribution and then clipped to remain within valid bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Neural Network Adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's start with the actor network:\n",
    "\n",
    "```python\n",
    "\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "\n",
    "\n",
    "        class ActorNetwork(nn.Module):\n",
    "        \"\"\"\n",
    "        Actor network for continuous actions.\n",
    "        \n",
    "        This network processes the input state and outputs the mean values\n",
    "        for each action dimension. A separate parameter is used to learn the\n",
    "        log standard deviation.\n",
    "        \"\"\"\n",
    "        def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "                super().__init__()\n",
    "                input_dim = int(np.prod(state_shape))\n",
    "                output_dim = int(np.prod(action_shape))\n",
    "                \n",
    "                # Define a sequential network to output the action means.\n",
    "                self.actor = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.LayerNorm(hidden_size),\n",
    "                nn.Linear(hidden_size, output_dim)\n",
    "                )\n",
    "\n",
    "```\n",
    "\n",
    "This model can remain the same, as it will output a continuous value for each action space. We'll use this value as the mean of a Gaussian distribution. \n",
    "\n",
    "We'll then have to add an additional parameter for the standard deviation of the Gaussian distribution.\n",
    "\n",
    "```python\n",
    "                # Define a learnable parameter for log standard deviation.\n",
    "                # Clipping is performed later in the forward pass for numerical stability.\n",
    "                self.actor_log_std = nn.Parameter(torch.zeros(output_dim), requires_grad=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next in the forward pass, where our model is used to make predictions about which actions to take, we'll have to modify how those actions are chosen.\n",
    "\n",
    "```python\n",
    "        def forward(self, state):\n",
    "                \"\"\"\n",
    "                Perform a forward pass through the actor network.\n",
    "                \n",
    "                Parameters:\n",
    "                state (torch.Tensor): The input state.\n",
    "                \n",
    "                Returns:\n",
    "                action_mean (torch.Tensor): Mean of the Gaussian distribution for each action.\n",
    "                action_std (torch.Tensor): Standard deviation for each action.\n",
    "                \"\"\"\n",
    "                # Compute the mean values from the actor network.\n",
    "                action_mean = self.actor(state)\n",
    "                # Clamp log_std for numerical stability, then exponentiate.\n",
    "                action_log_std = self.actor_log_std.clamp(-20, 2)\n",
    "                action_std = action_log_std.exp()\n",
    "```\n",
    "\n",
    "Then when we return the action choice we keep both the mean and the std of the actions:\n",
    "\n",
    "```python\n",
    "                return action_mean, action_std\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Policy Adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When taking actions we'll have to adjust how these actions are chosen:\n",
    "\n",
    "```python\n",
    "        import torch\n",
    "        from torch.distributions import Normal\n",
    "        from tianshou.data import Batch\n",
    "        \n",
    "        def policy_forward(model, batch, action_space):\n",
    "        \"\"\"\n",
    "        Forward pass for the policy.\n",
    "        \n",
    "        Parameters:\n",
    "                model (nn.Module): The actor-critic network.\n",
    "                batch (Batch): A batch of data from the environment.\n",
    "                action_space (gym.Space): The action space to clip actions.\n",
    "        \n",
    "        Returns:\n",
    "                Batch: Contains the sampled actions and the distribution.\n",
    "        \"\"\"\n",
    "        # Forward pass through the model to obtain mean and std for the Gaussian.\n",
    "        action_mean, action_std, _ = model(batch.obs)\n",
    "        \n",
    "        # Create a Gaussian distribution with the obtained parameters.\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        \n",
    "        # Sample an action from the distribution.\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Clip actions to ensure they are within the environment's bounds.\n",
    "        # Convert action space bounds to tensors for compatibility.\n",
    "        action_min = torch.tensor(action_space.low, dtype=torch.float32, device=action.device)\n",
    "        action_max = torch.tensor(action_space.high, dtype=torch.float32, device=action.device)\n",
    "        action = torch.clamp(action, action_min, action_max)\n",
    "        \n",
    "        return Batch(act=action.cpu().numpy(), dist=dist)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the learning section of the policy we need to calculate the log probability of each action in a slightly different way now that we have continuous actions.\n",
    "\n",
    "```python\n",
    "    def policy_learn(model, optim, batch, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Learn method for the continuous action policy.\n",
    "        \n",
    "        This method computes the loss for both the policy (actor) and value function (critic),\n",
    "        then performs backpropagation to update the network parameters.\n",
    "        \n",
    "        Parameters:\n",
    "            model (nn.Module): The actor-critic network.\n",
    "            optim (torch.optim.Optimizer): The optimizer.\n",
    "            batch (Batch): A batch of transitions from the replay buffer.\n",
    "            gamma (float): Discount factor for future rewards.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary with loss components.\n",
    "        \"\"\"\n",
    "        # Forward pass to get mean, std, and state value.\n",
    "        action_mean, action_std, state_values = model(batch.obs)\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        \n",
    "        # Compute log probabilities of the taken actions. Sum over action dimensions.\n",
    "        log_probs = dist.log_prob(batch.act).sum(dim=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## **Full Implementation and Testing**: : Continuous A2C\n",
    "\n",
    "Below, we integrate the neural network and policy adjustments into a full Actor-Critic model. We will define our actor-critic network, the A2C policy, and then run a training loop on the \"MountainCarContinuous-v0\" environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preliminary Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "\n",
    "# Timestamped ID to avoid overwriting previous runs.\n",
    "agent_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Format: YYYYMMDD_HHMMSS\n",
    "\n",
    "# Setup directories for models and logs.\n",
    "continuous_a2c_dir = f\"continuous_a2c_run_{agent_id}\"\n",
    "logs_dir = os.path.join(continuous_a2c_dir, \"logs\")\n",
    "models_dir = os.path.join(continuous_a2c_dir, \"models\")\n",
    "\n",
    "os.makedirs(continuous_a2c_dir, exist_ok=True)\n",
    "print(f\"Run files will be saved in: {continuous_a2c_dir}\")\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "print(f\"TensorBoard logs will be saved in: {logs_dir}\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "print(f\"Models will be saved in: {models_dir}\")\n",
    "\n",
    "# Create a TensorBoard logger.\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter(logs_dir))\n",
    "print(f\"TensorBoard logging is active.\")\n",
    "\n",
    "# Select the appropriate device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Actor-Critic Network for Continuous Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "from tianshou.data import Batch\n",
    "\n",
    "class ActorCriticNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Actor-Critic network for continuous action environments.\n",
    "\n",
    "    The actor branch outputs the mean and standard deviation for a Gaussian distribution,\n",
    "    from which actions are sampled. The critic branch outputs a scalar value estimate for\n",
    "    the current state.\n",
    "\n",
    "    This network is callable, meaning that calling an instance will invoke the forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_shape, action_shape, hidden_size=128):\n",
    "        super().__init__()\n",
    "        input_dim = int(np.prod(state_shape))\n",
    "        output_dim = int(np.prod(action_shape))\n",
    "        \n",
    "        # Actor network: processes the input state and produces the mean of the Gaussian.\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),  # Input to hidden layer.\n",
    "            nn.ReLU(),                          # Non-linear activation.\n",
    "            nn.LayerNorm(hidden_size),          # Normalization for stable training.\n",
    "            nn.Linear(hidden_size, hidden_size),# Second hidden layer.\n",
    "            nn.ReLU(),                          # Activation function.\n",
    "            nn.LayerNorm(hidden_size),          # Normalization layer.\n",
    "            nn.Linear(hidden_size, output_dim)  # Output layer for action mean.\n",
    "        )\n",
    "        \n",
    "        # Log standard deviation as a learnable parameter.\n",
    "        # We use log_std to ensure the standard deviation remains positive after exponentiation.\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(output_dim), requires_grad=True)\n",
    "        \n",
    "        # Critic network: processes the input state to estimate its value.\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),  # Input to hidden layer.\n",
    "            nn.ReLU(),                          # Activation function.\n",
    "            nn.LayerNorm(hidden_size),          # Normalization for stability.\n",
    "            nn.Linear(hidden_size, hidden_size),# Second hidden layer.\n",
    "            nn.ReLU(),                          # Activation function.\n",
    "            nn.LayerNorm(hidden_size),          # Normalization layer.\n",
    "            nn.Linear(hidden_size, 1)           # Output layer for state value.\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, state=None, info={}):\n",
    "        \"\"\"\n",
    "        Forward pass for the Actor-Critic network.\n",
    "\n",
    "        Parameters:\n",
    "            obs (np.ndarray or torch.Tensor): The input state observations.\n",
    "            state (optional): Additional state information (if any).\n",
    "            info (dict, optional): Additional information (if any).\n",
    "\n",
    "        Returns:\n",
    "            tuple: (action_mean, action_std, state_value)\n",
    "                - action_mean: Mean values for the Gaussian distribution of each action.\n",
    "                - action_std: Standard deviation for each action.\n",
    "                - state_value: Scalar value estimation of the current state.\n",
    "        \"\"\"\n",
    "        # Convert numpy array input to torch tensor if necessary.\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float32, device=next(self.parameters()).device)\n",
    "        \n",
    "        # Compute the mean for actions using the actor network.\n",
    "        action_mean = self.actor(obs)\n",
    "        # Clamp log_std for numerical stability and compute the standard deviation.\n",
    "        action_log_std = self.actor_log_std.clamp(-20, 2)\n",
    "        action_std = action_log_std.exp()\n",
    "        # Compute the state value using the critic network.\n",
    "        state_value = self.critic(obs).squeeze(-1)\n",
    "        return action_mean, action_std, state_value\n",
    "\n",
    "    def __call__(self, obs, state=None, info={}):\n",
    "        \"\"\"\n",
    "        Enables the network to be called like a function.\n",
    "\n",
    "        This method delegates the call to the forward method.\n",
    "\n",
    "        Parameters:\n",
    "            obs (np.ndarray or torch.Tensor): The input state observations.\n",
    "            state (optional): Additional state information.\n",
    "            info (dict, optional): Additional information.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (action_mean, action_std, state_value) as defined in forward().\n",
    "        \"\"\"\n",
    "        return self.forward(obs, state, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Advantage Actor-Critic (A2C) Policy for Continuous Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "from tianshou.data import Batch\n",
    "\n",
    "class A2CPolicy:\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic (A2C) policy for continuous actions.\n",
    "\n",
    "    This policy class provides methods for selecting actions and updating the model\n",
    "    based on collected experiences. It is made callable and includes a map_action method\n",
    "    to integrate seamlessly with the latest Tianshou Collector API.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, optim, action_space, gamma=0.99):\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.action_space = action_space\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, batch, state=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform a forward pass to select an action.\n",
    "\n",
    "        Parameters:\n",
    "            batch (Batch): Contains the observation data.\n",
    "            state (optional): Additional state information.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            Batch: A batch containing the sampled action (in numpy format)\n",
    "                   and its associated distribution.\n",
    "        \"\"\"\n",
    "        # Get the action mean and standard deviation from the model.\n",
    "        action_mean, action_std, _ = self.model(batch.obs)\n",
    "        # Create a Gaussian distribution with the obtained parameters.\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        # Sample an action from the distribution.\n",
    "        action = dist.sample()\n",
    "\n",
    "        # Convert the action space bounds to torch tensors and clip the action.\n",
    "        action_min = torch.tensor(self.action_space.low, dtype=torch.float32, device=action.device)\n",
    "        action_max = torch.tensor(self.action_space.high, dtype=torch.float32, device=action.device)\n",
    "        action = torch.clamp(action, action_min, action_max)\n",
    "\n",
    "        return Batch(act=action.cpu().numpy(), dist=dist)\n",
    "\n",
    "    def learn(self, batch, **kwargs):\n",
    "        \"\"\"\n",
    "        Update the policy using a batch of transitions.\n",
    "\n",
    "        The method calculates the loss for both the actor and critic components,\n",
    "        performs backpropagation, and updates the model parameters.\n",
    "\n",
    "        Parameters:\n",
    "            batch (Batch): A batch of transitions with observations, actions, rewards, etc.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the overall loss and its components:\n",
    "                  \"loss\", \"policy_loss\", and \"value_loss\".\n",
    "        \"\"\"\n",
    "        # Forward pass to get current predictions.\n",
    "        action_mean, action_std, state_values = self.model(batch.obs)\n",
    "        # Create a Gaussian distribution to compute log probabilities.\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        # Calculate log probabilities for the taken actions (summing over action dimensions).\n",
    "        log_probs = dist.log_prob(batch.act).sum(dim=-1)\n",
    "\n",
    "        # Compute the Temporal Difference (TD) target using next state values.\n",
    "        with torch.no_grad():\n",
    "            _, _, next_state_values = self.model(batch.obs_next)\n",
    "            td_target = batch.rew + self.gamma * (1 - batch.done) * next_state_values\n",
    "            # Calculate and normalize the advantage.\n",
    "            advantage = td_target - state_values\n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "\n",
    "        # Compute the entropy for encouraging exploration.\n",
    "        entropy = dist.entropy().mean()\n",
    "        # Actor loss: encourages actions with higher advantage, includes entropy regularization.\n",
    "        policy_loss = -(log_probs * advantage.detach()).mean() - 0.01 * entropy\n",
    "        # Critic loss: mean squared error between predicted state value and TD target.\n",
    "        value_loss = nn.functional.mse_loss(state_values, td_target)\n",
    "        # Total loss is a combination of both.\n",
    "        loss = policy_loss + value_loss\n",
    "\n",
    "        # Perform backpropagation and update the network parameters.\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
    "        self.optim.step()\n",
    "\n",
    "        return {\"loss\": loss.item(), \"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item()}\n",
    "\n",
    "    def map_action(self, action):\n",
    "        \"\"\"\n",
    "        Maps the raw action output to the environment's action space.\n",
    "        For continuous actions, this is an identity mapping.\n",
    "\n",
    "        Parameters:\n",
    "            action: The raw action output.\n",
    "\n",
    "        Returns:\n",
    "            The action, unchanged.\n",
    "        \"\"\"\n",
    "        return action\n",
    "\n",
    "    def __call__(self, batch, state=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Makes the policy callable, enabling it to be used directly by Tianshou's Collector.\n",
    "\n",
    "        This method delegates the call to the forward method.\n",
    "\n",
    "        Parameters:\n",
    "            batch (Batch): Contains the observation data.\n",
    "            state (optional): Additional state information.\n",
    "            **kwargs: Additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            Batch: The output from the forward method.\n",
    "        \"\"\"\n",
    "        return self.forward(batch, state, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create the \"MountainCarContinuous-v0\" Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create a single environment to access space details.\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "state_shape = env.observation_space.shape\n",
    "action_shape = env.action_space.shape\n",
    "action_space = env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Initialize the Actor-Critic Network, A2C Policy, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "net = ActorCriticNet(state_shape, action_shape).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "policy = A2CPolicy(model=net, optim=optimizer, action_space=action_space, gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training on MountainCarContinuous-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.data import ReplayBuffer, Collector, Batch\n",
    "import tianshou as ts\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "# === TensorBoard Setup ===\n",
    "def kill_port(port):\n",
    "    \"\"\"\n",
    "    Terminates any processes that are listening on the specified port.\n",
    "    Works on both Unix-based systems and Windows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.name == 'nt':\n",
    "            # Windows: Use netstat and taskkill to kill processes on the given port.\n",
    "            cmd = f'for /f \"tokens=5\" %a in (\\'netstat -aon ^| findstr :{port}\\') do taskkill /F /PID %a'\n",
    "            subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Killed processes on port {port}.\")\n",
    "        else:\n",
    "            # Unix (Linux/Mac): Use lsof to find processes on the port and kill them.\n",
    "            cmd = f\"lsof -ti:{port} | xargs kill -9\"\n",
    "            subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Killed processes on port {port}.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        if \"returned non-zero exit status 1\" in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Could not kill process on port {port}: {e}\")\n",
    "\n",
    "# Kill any processes on port 6006 to ensure it is free.\n",
    "kill_port(6006)\n",
    "\n",
    "# Ensure that logs_dir is defined from our run setup cell.\n",
    "# (logs_dir was defined earlier in the setup_run.py cell.)\n",
    "if 'logs_dir' not in globals():\n",
    "    logs_dir = \"./logs\"  # Fallback path\n",
    "\n",
    "# Clear previous TensorBoard sessions.\n",
    "tensorboard_info = os.path.join(tempfile.gettempdir(), \".tensorboard-info\")\n",
    "if os.path.exists(tensorboard_info):\n",
    "    shutil.rmtree(tensorboard_info)\n",
    "\n",
    "# Launch TensorBoard in the background on port 6006.\n",
    "tb_command = [\n",
    "    \"tensorboard\",\n",
    "    \"--logdir\", logs_dir,\n",
    "    \"--port\", \"6006\",\n",
    "    \"--host\", \"localhost\",\n",
    "    \"--reload_interval\", \"30\"\n",
    "]\n",
    "tb_process = subprocess.Popen(tb_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# Allow time for TensorBoard to start and display its dashboard.\n",
    "time.sleep(5)\n",
    "display(IFrame(src=\"http://localhost:6006\", width=\"100%\", height=\"800\"))\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# === Training Hyperparameters and Setup ===\n",
    "max_epoch = 10               # Total number of epochs for training.\n",
    "steps_per_epoch = 10000      # Number of training steps per epoch.\n",
    "keep_n_steps = 30            # Number of recent transitions to use for learning.\n",
    "\n",
    "# Create a ReplayBuffer to store transitions.\n",
    "buffer = ReplayBuffer(size=keep_n_steps)\n",
    "\n",
    "# Create collectors for training and testing.\n",
    "train_collector = Collector(policy, env, buffer)\n",
    "test_collector = Collector(policy, env)\n",
    "\n",
    "# Lists to store summaries for analysis.\n",
    "epoch_training_losses = []\n",
    "epoch_test_rewards = []\n",
    "epoch_durations = []\n",
    "\n",
    "global_start_time = time.time()  # Start overall training timer.\n",
    "\n",
    "# === Training Loop with Detailed Progress Tracking ===\n",
    "for epoch in range(max_epoch):\n",
    "    epoch_start_time = time.time()  # Timer for the current epoch.\n",
    "    train_collector.reset()         # Reset collector at the start of each epoch.\n",
    "    running_loss = 0.0              # Accumulate loss to compute average loss.\n",
    "\n",
    "    # Set up a tqdm progress bar with dynamic post-fix metrics.\n",
    "    progress_bar = tqdm(\n",
    "        range(steps_per_epoch),\n",
    "        desc=f\"Epoch {epoch+1}/{max_epoch}\",\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "    \n",
    "    for step in progress_bar:\n",
    "        # Collect a fixed number of steps and store transitions in the buffer.\n",
    "        train_collector.collect(n_step=keep_n_steps)\n",
    "        # Retrieve the most recent transitions.\n",
    "        batch = train_collector.buffer[-keep_n_steps:]\n",
    "        \n",
    "        # Convert batch fields to torch tensors for compatibility.\n",
    "        batch.obs = torch.tensor(batch.obs, dtype=torch.float32)\n",
    "        batch.act = torch.tensor(batch.act, dtype=torch.float32)  # Continuous actions use float type.\n",
    "        batch.rew = torch.tensor(batch.rew, dtype=torch.float32)\n",
    "        batch.done = torch.tensor(batch.done, dtype=torch.float32)\n",
    "        batch.obs_next = torch.tensor(batch.obs_next, dtype=torch.float32)\n",
    "        \n",
    "        # Update the policy using the collected batch and capture the loss.\n",
    "        learn_info = policy.learn(batch)\n",
    "        loss_val = learn_info.get(\"loss\", 0)\n",
    "        running_loss += loss_val\n",
    "        \n",
    "        global_step = epoch * steps_per_epoch + step\n",
    "        \n",
    "        # Log step-level metrics to TensorBoard.\n",
    "        logger.writer.add_scalar(\"Loss/train_step\", loss_val, global_step)\n",
    "        logger.writer.add_scalar(\"Loss/train_running_avg\", running_loss / (step + 1), global_step)\n",
    "        \n",
    "        # Flush logs periodically.\n",
    "        if step % 50 == 0:\n",
    "            logger.writer.flush()\n",
    "        \n",
    "        # Update progress bar with current metrics.\n",
    "        progress_bar.set_postfix({\n",
    "            \"Step\": f\"{step}/{steps_per_epoch}\",\n",
    "            \"Loss\": f\"{loss_val:07.3f}\",\n",
    "            \"AvgLoss\": f\"{running_loss / (step + 1):07.3f}\"\n",
    "        })\n",
    "        \n",
    "        # Print summary at every 25% of the epoch.\n",
    "        if step % (steps_per_epoch // 4) == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}, Step {step}/{steps_per_epoch}: \"\n",
    "                f\"Step Loss = {loss_val}, Running Avg Loss = {running_loss / (step + 1)}\"\n",
    "            )\n",
    "    \n",
    "    # Compute average loss over the epoch.\n",
    "    avg_loss = running_loss / steps_per_epoch\n",
    "\n",
    "    # Evaluate the agent on 10 episodes.\n",
    "    test_collector.reset()\n",
    "    test_result = test_collector.collect(n_episode=10)\n",
    "    mean_reward = np.mean(test_result[\"rews\"])\n",
    "    std_reward = np.std(test_result[\"rews\"])\n",
    "    min_reward = np.min(test_result[\"rews\"])\n",
    "    p25_reward = np.percentile(test_result[\"rews\"], 25)\n",
    "    median_reward = np.median(test_result[\"rews\"])\n",
    "    p75_reward = np.percentile(test_result[\"rews\"], 75)\n",
    "    max_reward = np.max(test_result[\"rews\"])\n",
    "\n",
    "    # Log epoch-level metrics to TensorBoard.\n",
    "    logger.writer.add_scalar(\"Reward/test_avg\", mean_reward, epoch)\n",
    "    logger.writer.add_scalar(\"Loss/train_avg\", avg_loss, epoch)\n",
    "    logger.writer.flush()\n",
    "\n",
    "    # Calculate epoch elapsed time.\n",
    "    epoch_elapsed = time.time() - epoch_start_time\n",
    "    epoch_training_losses.append(avg_loss)\n",
    "    epoch_test_rewards.append(mean_reward)\n",
    "    epoch_durations.append(epoch_elapsed)\n",
    "\n",
    "    # Print detailed epoch summary.\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch+1} Summary:\\n\"\n",
    "        f\"  - Epoch Elapsed Time      : {epoch_elapsed} seconds\\n\"\n",
    "        f\"  - Steps Collected         : {steps_per_epoch}\\n\"\n",
    "        f\"  - Average Training Loss   : {avg_loss}\\n\"\n",
    "        f\"  - Mean Test Reward        : {mean_reward}\\n\"\n",
    "        f\"  - Std Test Reward         : {std_reward}\\n\"\n",
    "        f\"  - Min Test Reward         : {min_reward}\\n\"\n",
    "        f\"  - 25th Percentile Reward  : {p25_reward}\\n\"\n",
    "        f\"  - Median Test Reward      : {median_reward}\\n\"\n",
    "        f\"  - 75th Percentile Reward  : {p75_reward}\\n\"\n",
    "        f\"  - Max Test Reward         : {max_reward}\\n\"\n",
    "    )\n",
    "\n",
    "# Final flush and close the TensorBoard writer.\n",
    "logger.writer.close()\n",
    "\n",
    "# Overall training statistics.\n",
    "total_elapsed = time.time() - global_start_time\n",
    "overall_avg_loss = np.mean(epoch_training_losses)\n",
    "overall_avg_reward = np.mean(epoch_test_rewards)\n",
    "total_epochs = len(epoch_durations)\n",
    "\n",
    "print(\"\\nOverall Training Summary:\")\n",
    "print(f\"  - Total Epochs            : {total_epochs}\")\n",
    "print(f\"  - Overall Average Loss    : {overall_avg_loss}\")\n",
    "print(f\"  - Overall Average Reward  : {overall_avg_reward}\")\n",
    "print(f\"  - Total Elapsed Time      : {total_elapsed} seconds\")\n",
    "\n",
    "print(\"\\nFinal Epoch Summary:\")\n",
    "print(\n",
    "    f\"  - Epoch {total_epochs}:\\n\"\n",
    "    f\"      * Average Training Loss : {epoch_training_losses[-1]}\\n\"\n",
    "    f\"      * Average Test Reward   : {epoch_test_rewards[-1]}\\n\"\n",
    "    f\"      * Epoch Elapsed Time    : {epoch_durations[-1]} seconds\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it learn? Do you see rewards increasing? \n",
    "\n",
    "If so, let's save the model's state dictionary. This allows you to reload the trained agent later without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Save the model's state dictionary for future use.\n",
    "model_path = os.path.join(models_dir, f\"A2C_mountain_model_{agent_id}.pth\")\n",
    "torch.save(net.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Testing the Trained Model\n",
    "\n",
    "First, we load the saved Actor-Critic model and prepares it for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "model_path = os.path.join(models_dir, f\"A2C_mountain_model_{agent_id}.pth\")\n",
    "\n",
    "# Initialize a new network with the same architecture and load saved parameters.\n",
    "loaded_net = ActorCriticNet(state_shape, action_shape).to(device)\n",
    "loaded_net.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an evaluation environment and build a policy based on the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    " # Set to True to record the evaluation video, False to render in human mode.\n",
    "output_video = False\n",
    "\n",
    "# Create the evaluation environment.\n",
    "if output_video:\n",
    "    eval_env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"rgb_array\")\n",
    "else:\n",
    "    eval_env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"human\")\n",
    "\n",
    "loaded_policy = A2CPolicy(model=loaded_net, optim=optimizer, action_space=action_space, gamma=0.99)\n",
    "print(\"Evaluation environment and policy set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run our agent in the environment. **Note**: You can change the number of episodes to watch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from tianshou.data import Batch\n",
    "\n",
    "num_episodes = 20       # Number of evaluation episodes.\n",
    "frames = []             # List to store frames for video recording.\n",
    "episode_rewards = []    # List to store total rewards per episode.\n",
    "episode_lengths = []    # List to store the number of steps per episode.\n",
    "\n",
    "# Loop over each evaluation episode.\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = eval_env.reset()  # Reset the environment and get the initial observation.\n",
    "    done = False               # Flag to track the end of an episode.\n",
    "    total_reward = 0           # Accumulate the total reward for the episode.\n",
    "    step_count = 0             # Counter for the number of steps.\n",
    "    print(f\"Starting episode {episode + 1}\")\n",
    "    \n",
    "    # Run the episode until termination.\n",
    "    while not done:\n",
    "        # Create a Batch object from the current observation.\n",
    "        obs_batch = Batch(obs=[obs])\n",
    "        # Get the action from the loaded policy.\n",
    "        action = loaded_policy.forward(obs_batch).act[0]\n",
    "        # Apply the action to the environment.\n",
    "        obs, reward, done, truncated, _ = eval_env.step(action)\n",
    "        total_reward += reward  # Accumulate reward.\n",
    "        step_count += 1         # Increment the step counter.\n",
    "        frames.append(eval_env.render())  # Record the current frame.\n",
    "        \n",
    "        # End the episode if finished.\n",
    "        if done or truncated:\n",
    "            print(f\"Episode {episode + 1} ended with total reward: {total_reward} after {step_count} steps.\")\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(step_count)\n",
    "\n",
    "# Convert lists to numpy arrays for statistical calculations.\n",
    "episode_rewards = np.array(episode_rewards)\n",
    "episode_lengths = np.array(episode_lengths)\n",
    "\n",
    "# Compute and print comprehensive performance statistics.\n",
    "if episode_rewards.size > 0 and episode_lengths.size > 0:\n",
    "    count_rewards = len(episode_rewards)\n",
    "    mean_rewards = np.mean(episode_rewards)\n",
    "    std_rewards = np.std(episode_rewards)\n",
    "    min_rewards = np.min(episode_rewards)\n",
    "    p25_rewards = np.percentile(episode_rewards, 25)\n",
    "    median_rewards = np.median(episode_rewards)\n",
    "    p75_rewards = np.percentile(episode_rewards, 75)\n",
    "    max_rewards = np.max(episode_rewards)\n",
    "    \n",
    "    count_lengths = len(episode_lengths)\n",
    "    mean_lengths = np.mean(episode_lengths)\n",
    "    std_lengths = np.std(episode_lengths)\n",
    "    min_lengths = np.min(episode_lengths)\n",
    "    p25_lengths = np.percentile(episode_lengths, 25)\n",
    "    median_lengths = np.median(episode_lengths)\n",
    "    p75_lengths = np.percentile(episode_lengths, 75)\n",
    "    max_lengths = np.max(episode_lengths)\n",
    "    \n",
    "    print(\"\\nFinal Evaluation Performance Summary:\")\n",
    "    print(f\"Total Episodes Evaluated: {num_episodes}\\n\")\n",
    "    header = \"{:<22} {:>15} {:>20}\".format(\"Statistic\", \"Rewards\", \"Episode Lengths\")\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    print(\"{:<22} {:>15d} {:>20d}\".format(\"Count\", count_rewards, count_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Mean\", mean_rewards, mean_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Std Dev\", std_rewards, std_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Min\", min_rewards, min_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"25th Percentile\", p25_rewards, p25_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Median\", median_rewards, median_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"75th Percentile\", p75_rewards, p75_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Max\", max_rewards, max_lengths))\n",
    "else:\n",
    "    print(\"No performance data was collected. Please verify the Collector configuration.\")\n",
    "\n",
    "# Close the environment after evaluation.\n",
    "eval_env.close()\n",
    "print(\"Evaluation completed and environment closed.\")\n",
    "\n",
    "try:\n",
    "    # Save the recorded frames as a video file.\n",
    "    video_path = os.path.join(continuous_a2c_dir, f\"mountain_car_simulation_{agent_id}.mp4\")\n",
    "    imageio.mimsave(video_path, frames, fps=60)\n",
    "    print(f\"Simulation video saved as {video_path}\")\n",
    "except Exception as e:\n",
    "    print(\"If a video is intended, make sure that the rendering mode is set to 'rgb_array' in the environment.\")\n",
    "    print(f\"Otherwise, an error occurred while saving the simulation video: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Things to Try**\n",
    "\n",
    "Try changing the environment or changing the hyperparameters:\n",
    "\n",
    "- Experiment with different environments (e.g., `Pendulum-v1`, `BipedalWalker-v3`) to see how the continuous A2C performs.\n",
    "- Tweak hyperparameters such as the learning rate and discount factor. Note that a high learning rate may cause the agent to pick up spurious patterns, while a low rate might slow down the learning.\n",
    "- Observe the rewards: Do they increase steadily? What hyperparameters seem to work best for your specific environment?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Ant‑v5 Environment\n",
    "### Training an Agent in a More Complex Environment\n",
    "\n",
    "Now that our agent can handle continuous actions, we can train it in a more challenging environment. For example, the `Ant-v5` environment requires the agent to control a multi-legged robot. This is also an opportunity to explore more complex body dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the code below to make sure all is working well!\n",
    "It should create the \"Hopper\" environment and print out the observation space, and action spaces. \n",
    "You can read up on the Hopper environment, and see the actions the agent can take, the observations it can see, and the rewards it can acheive. You'll note they are continuous actions!\n",
    "* https://gymnasium.farama.org/environments/mujoco/hopper/\n",
    "\n",
    "We now verify that the Ant‑v5 environment is correctly installed and inspect its observation and action spaces.  \n",
    "Understanding these spaces is essential, as they define what the agent can \"see\" (observations) and how it can \"act\" (actions) in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# Create the Ant-v5 environment.\n",
    "env_ant = gym.make(\"Ant-v5\")\n",
    "\n",
    "# Print the observation space and action space.\n",
    "print(\"Observation space:\", env_ant.observation_space)\n",
    "print(\"Action space:\", env_ant.action_space)\n",
    "\n",
    "# Close the environment to free resources.\n",
    "env_ant.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The observation space tells you what state features are available (for Ant-v5, a Box with shape (27,)).\n",
    "* The action space shows the allowed actions (for Ant-v5, a Box with shape (8,) with values typically between -1 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preliminary Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "\n",
    "# Timestamped ID to avoid overwriting previous runs.\n",
    "agent_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Format: YYYYMMDD_HHMMSS\n",
    "\n",
    "# Setup directories for models and logs.\n",
    "a2c_ant_agent_dir = f\"a2c_ant_agent_{agent_id}\"\n",
    "logs_dir = os.path.join(a2c_ant_agent_dir, \"logs\")\n",
    "models_dir = os.path.join(a2c_ant_agent_dir, \"models\")\n",
    "\n",
    "os.makedirs(a2c_ant_agent_dir, exist_ok=True)\n",
    "print(f\"Run files will be saved in: {a2c_ant_agent_dir}\")\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "print(f\"TensorBoard logs will be saved in: {logs_dir}\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "print(f\"Models will be saved in: {models_dir}\")\n",
    "\n",
    "# Create a TensorBoard logger.\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter(logs_dir))\n",
    "print(f\"TensorBoard logging is active.\")\n",
    "\n",
    "# Select the appropriate device.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                      \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting Up the Agent for Ant-v5\n",
    "\n",
    " we set up the environment and initialize our Actor-Critic network and A2C policy for the Ant‑v5 environment.  \n",
    "The agent uses the observation and action spaces to configure its neural network, which outputs parameters for a Gaussian distribution over actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then use our training code to train a Hopper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from torch import optim\n",
    "\n",
    "# Create a single instance of the Ant-v5 environment to extract its properties.\n",
    "single_env_ant = gym.make(\"Ant-v5\")\n",
    "state_shape = single_env_ant.observation_space.shape    # Shape of the observation vector.\n",
    "action_shape = single_env_ant.action_space.shape        # Shape of the action vector.\n",
    "action_space = single_env_ant.action_space              # The action space (used for clipping actions).\n",
    "\n",
    "# Initialize the Actor-Critic network for Ant-v5 and move it to the appropriate device.\n",
    "net_ant = ActorCriticNet(state_shape, action_shape).to(device)\n",
    "\n",
    "# Use an optimizer with a slightly higher learning rate due to the increased complexity of Ant-v5.\n",
    "optimizer_ant = optim.Adam(net_ant.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Create the A2C policy using the network, optimizer, and action space.\n",
    "policy_ant = A2CPolicy(model=net_ant, optim=optimizer_ant, action_space=action_space, gamma=0.99)\n",
    "\n",
    "# Close the temporary environment instance.\n",
    "single_env_ant.close()\n",
    "print(\"Ant-v5 setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the Ant Agent (Ant-v5)\n",
    "\n",
    "Let's now train our Ant!\n",
    "\n",
    "Note: this training will take some time. You should be starting to see training taking more and more time as we build our agents towards a modern RL-agent. We'll start to talk more about the kinds of hardware you can access (i.e., free GPU on Google Colab), and how hardware becomes ever more important as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.data import ReplayBuffer, Collector, Batch\n",
    "import tianshou as ts\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "# === TensorBoard Setup ===\n",
    "def kill_port(port):\n",
    "    \"\"\"\n",
    "    Terminates any processes that are listening on the specified port.\n",
    "    Works on both Unix-based systems and Windows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.name == 'nt':\n",
    "            cmd = f'for /f \"tokens=5\" %a in (\\'netstat -aon ^| findstr :{port}\\') do taskkill /F /PID %a'\n",
    "            subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Killed processes on port {port}.\")\n",
    "        else:\n",
    "            cmd = f\"lsof -ti:{port} | xargs kill -9\"\n",
    "            subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            print(f\"Killed processes on port {port}.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        if \"returned non-zero exit status 1\" in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"Could not kill process on port {port}: {e}\")\n",
    "\n",
    "kill_port(6006)\n",
    "\n",
    "# Use logs_dir defined in the setup_run cell, or use a fallback.\n",
    "if 'logs_dir' not in globals():\n",
    "    logs_dir = \"./logs\"\n",
    "\n",
    "# Clear previous TensorBoard sessions.\n",
    "tensorboard_info = os.path.join(tempfile.gettempdir(), \".tensorboard-info\")\n",
    "if os.path.exists(tensorboard_info):\n",
    "    shutil.rmtree(tensorboard_info)\n",
    "\n",
    "# Launch TensorBoard in the background on port 6006.\n",
    "tb_command = [\n",
    "    \"tensorboard\",\n",
    "    \"--logdir\", logs_dir,\n",
    "    \"--port\", \"6006\",\n",
    "    \"--host\", \"localhost\",\n",
    "    \"--reload_interval\", \"30\"\n",
    "]\n",
    "tb_process = subprocess.Popen(tb_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "time.sleep(5)\n",
    "display(IFrame(src=\"http://localhost:6006\", width=\"100%\", height=\"800\"))\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# === Training Hyperparameters and Setup for Ant-v5 ===\n",
    "max_epoch = 10             # Total number of training epochs.\n",
    "steps_per_epoch = 1000     # Number of training steps per epoch.\n",
    "keep_n_steps = 200         # Number of transitions to collect per update.\n",
    "\n",
    "# Create a ReplayBuffer to store transitions.\n",
    "buffer_ant = ReplayBuffer(size=keep_n_steps)\n",
    "\n",
    "# Create collectors for training and testing using the Ant-v5 environment.\n",
    "train_collector_ant = Collector(policy_ant, env_ant, buffer_ant)\n",
    "test_collector_ant = Collector(policy_ant, env_ant)\n",
    "\n",
    "# Lists to store training summaries for analysis.\n",
    "epoch_training_losses = []\n",
    "epoch_test_rewards = []\n",
    "epoch_durations = []\n",
    "\n",
    "global_start_time = time.time()  # Start overall training timer.\n",
    "\n",
    "# === Training Loop with Detailed Progress Tracking ===\n",
    "for epoch in range(max_epoch):\n",
    "    epoch_start_time = time.time()  # Timer for the current epoch.\n",
    "    train_collector_ant.reset()       # Reset the collector at the start of the epoch.\n",
    "    running_loss = 0.0                # Accumulate loss for averaging.\n",
    "\n",
    "    # Set up a tqdm progress bar for this epoch.\n",
    "    progress_bar = tqdm(\n",
    "        range(steps_per_epoch),\n",
    "        desc=f\"Ant Epoch {epoch+1}/{max_epoch}\",\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "    \n",
    "    for step in progress_bar:\n",
    "        # Collect a fixed number of transitions.\n",
    "        train_collector_ant.collect(n_step=keep_n_steps)\n",
    "        # Retrieve the latest batch of transitions.\n",
    "        batch = train_collector_ant.buffer[-keep_n_steps:]\n",
    "        \n",
    "        # Convert batch data to torch tensors.\n",
    "        batch.obs = torch.tensor(batch.obs, dtype=torch.float32, device=device)\n",
    "        batch.act = torch.tensor(batch.act, dtype=torch.float32, device=device)\n",
    "        batch.rew = torch.tensor(batch.rew, dtype=torch.float32, device=device)\n",
    "        batch.done = torch.tensor(batch.done, dtype=torch.float32, device=device)\n",
    "        batch.obs_next = torch.tensor(batch.obs_next, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Normalize rewards to stabilize training.\n",
    "        batch.rew = (batch.rew - batch.rew.mean()) / (batch.rew.std() + 1e-8)\n",
    "        \n",
    "        # Update the policy using the collected batch.\n",
    "        loss_dict = policy_ant.learn(batch)\n",
    "        loss_val = loss_dict.get(\"loss\", 0)\n",
    "        running_loss += loss_val\n",
    "        \n",
    "        global_step = epoch * steps_per_epoch + step\n",
    "        \n",
    "        # Log step-level metrics to TensorBoard.\n",
    "        logger.writer.add_scalar(\"Loss/train_step_ant\", loss_val, global_step)\n",
    "        logger.writer.add_scalar(\"Loss/train_running_avg_ant\", running_loss / (step + 1), global_step)\n",
    "        \n",
    "        if step % 50 == 0:\n",
    "            logger.writer.flush()\n",
    "        \n",
    "        # Update progress bar with current metrics.\n",
    "        progress_bar.set_postfix({\n",
    "            \"Step\": f\"{step}/{steps_per_epoch}\",\n",
    "            \"Loss\": f\"{loss_val:07.3f}\",\n",
    "            \"AvgLoss\": f\"{running_loss / (step + 1):07.3f}\"\n",
    "        })\n",
    "        \n",
    "        if step % (steps_per_epoch // 4) == 0:\n",
    "            print(\n",
    "                f\"Ant Epoch {epoch+1}, Step {step}/{steps_per_epoch}: \"\n",
    "                f\"Step Loss = {loss_val}, Running Avg Loss = {running_loss / (step + 1)}\"\n",
    "            )\n",
    "    \n",
    "    # Compute average loss for the epoch.\n",
    "    avg_loss = running_loss / steps_per_epoch\n",
    "\n",
    "    # Evaluate the agent on 10 episodes.\n",
    "    test_collector_ant.reset()\n",
    "    test_result = test_collector_ant.collect(n_episode=10)\n",
    "    mean_reward = np.mean(test_result[\"rews\"])\n",
    "    std_reward = np.std(test_result[\"rews\"])\n",
    "    min_reward = np.min(test_result[\"rews\"])\n",
    "    p25_reward = np.percentile(test_result[\"rews\"], 25)\n",
    "    median_reward = np.median(test_result[\"rews\"])\n",
    "    p75_reward = np.percentile(test_result[\"rews\"], 75)\n",
    "    max_reward = np.max(test_result[\"rews\"])\n",
    "\n",
    "    # Log epoch-level metrics to TensorBoard.\n",
    "    logger.writer.add_scalar(\"Reward/test_avg_ant\", mean_reward, epoch)\n",
    "    logger.writer.add_scalar(\"Loss/train_avg_ant\", avg_loss, epoch)\n",
    "    logger.writer.flush()\n",
    "\n",
    "    epoch_elapsed = time.time() - epoch_start_time\n",
    "    epoch_training_losses.append(avg_loss)\n",
    "    epoch_test_rewards.append(mean_reward)\n",
    "    epoch_durations.append(epoch_elapsed)\n",
    "\n",
    "    print(\n",
    "        f\"\\nAnt Epoch {epoch+1} Summary:\\n\"\n",
    "        f\"  - Epoch Elapsed Time      : {epoch_elapsed} seconds\\n\"\n",
    "        f\"  - Steps Collected         : {steps_per_epoch}\\n\"\n",
    "        f\"  - Average Training Loss   : {avg_loss}\\n\"\n",
    "        f\"  - Mean Test Reward        : {mean_reward}\\n\"\n",
    "        f\"  - Std Test Reward         : {std_reward}\\n\"\n",
    "        f\"  - Min Test Reward         : {min_reward}\\n\"\n",
    "        f\"  - 25th Percentile Reward  : {p25_reward}\\n\"\n",
    "        f\"  - Median Test Reward      : {median_reward}\\n\"\n",
    "        f\"  - 75th Percentile Reward  : {p75_reward}\\n\"\n",
    "        f\"  - Max Test Reward         : {max_reward}\\n\"\n",
    "    )\n",
    "\n",
    "# Final flush and close the TensorBoard writer.\n",
    "logger.writer.close()\n",
    "\n",
    "total_elapsed = time.time() - global_start_time\n",
    "overall_avg_loss = np.mean(epoch_training_losses)\n",
    "overall_avg_reward = np.mean(epoch_test_rewards)\n",
    "total_epochs = len(epoch_durations)\n",
    "\n",
    "print(\"\\nOverall Training Summary (Ant):\")\n",
    "print(f\"  - Total Epochs            : {total_epochs}\")\n",
    "print(f\"  - Overall Average Loss    : {overall_avg_loss}\")\n",
    "print(f\"  - Overall Average Reward  : {overall_avg_reward}\")\n",
    "print(f\"  - Total Elapsed Time      : {total_elapsed} seconds\")\n",
    "\n",
    "print(\"\\nFinal Epoch Summary (Ant):\")\n",
    "print(\n",
    "    f\"  - Epoch {total_epochs}:\\n\"\n",
    "    f\"      * Average Training Loss : {epoch_training_losses[-1]}\\n\"\n",
    "    f\"      * Average Test Reward   : {epoch_test_rewards[-1]}\\n\"\n",
    "    f\"      * Epoch Elapsed Time    : {epoch_durations[-1]} seconds\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it learn? Do you see rewards increasing? \n",
    "\n",
    "If so, let's save the model's state dictionary. This allows you to reload the trained agent later without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Save the model's state dictionary for future use.\n",
    "model_path = os.path.join(models_dir, f\"ant_v5_model_{agent_id}.pth\")\n",
    "torch.save(net_ant.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the model, and watch what it learnt.\n",
    "\n",
    "### 4. Setup the Evaluation Environment for Ant-v5\n",
    "\n",
    "Load in the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "model_path = os.path.join(models_dir, f\"ant_v5_model_{agent_id}.pth\")\n",
    "\n",
    "# Initialize a new network with the same architecture and load the saved parameters.\n",
    "loaded_net_ant = ActorCriticNet(state_shape, action_shape).to(device)\n",
    "loaded_net_ant.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print(\"Ant-v5 model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an environment and build a policy based on our saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    " # Set to True to record the evaluation video, False to render in human mode.\n",
    "output_video = False\n",
    "\n",
    "# Create the evaluation environment for Ant-v5 with rendering mode enabled.\n",
    "if output_video:\n",
    "    eval_env_ant = gym.make(\"Ant-v5\", render_mode=\"rgb_array\")\n",
    "else:\n",
    "    eval_env_ant = gym.make(\"Ant-v5\", render_mode=\"human\")\n",
    "\n",
    "# Build the evaluation policy using the loaded model.\n",
    "loaded_policy_ant = A2CPolicy(model=loaded_net_ant, optim=optimizer_ant, action_space=action_space, gamma=0.99)\n",
    "print(\"Ant-v5 evaluation environment and policy are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Running the Ant-v5 Agent and Recording a Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run our ant agent in the environment. We record each frame to create a simulation video. \n",
    "\n",
    "**Note**: You can change the number of episodes to watch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from tianshou.data import Batch\n",
    "\n",
    "num_episodes = 20       # Number of evaluation episodes.\n",
    "frames_ant = []         # List to store frames for the video.\n",
    "episode_rewards = []    # List to store total rewards per episode.\n",
    "episode_lengths = []    # List to store the number of steps per episode.\n",
    "\n",
    "# Loop over each evaluation episode.\n",
    "for episode in range(num_episodes):\n",
    "    obs, _ = eval_env_ant.reset()  # Reset the environment.\n",
    "    done = False                   # Flag to determine if the episode is finished.\n",
    "    total_reward = 0               # Initialize total reward.\n",
    "    step_count = 0                 # Initialize step counter.\n",
    "    print(f\"Starting episode {episode + 1}\")\n",
    "    \n",
    "    # Run the episode.\n",
    "    while not done:\n",
    "        # Create a Batch object from the current observation.\n",
    "        obs_batch = Batch(obs=[obs])\n",
    "        # Obtain the action from the loaded policy.\n",
    "        action = loaded_policy_ant.forward(obs_batch).act[0]\n",
    "        # Apply the action to the environment.\n",
    "        obs, reward, done, truncated, _ = eval_env_ant.step(action)\n",
    "        total_reward += reward  # Accumulate reward.\n",
    "        step_count += 1         # Increment step count.\n",
    "        frames_ant.append(eval_env_ant.render())  # Record the current frame.\n",
    "        \n",
    "        # End the episode if finished.\n",
    "        if done or truncated:\n",
    "            print(f\"Episode {episode + 1} ended with total reward: {total_reward} after {step_count} steps.\")\n",
    "            break\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(step_count)\n",
    "\n",
    "# Convert lists to numpy arrays for statistical calculations.\n",
    "episode_rewards = np.array(episode_rewards)\n",
    "episode_lengths = np.array(episode_lengths)\n",
    "\n",
    "# Compute and print comprehensive performance statistics.\n",
    "if episode_rewards.size > 0 and episode_lengths.size > 0:\n",
    "    count_rewards = len(episode_rewards)\n",
    "    mean_rewards = np.mean(episode_rewards)\n",
    "    std_rewards = np.std(episode_rewards)\n",
    "    min_rewards = np.min(episode_rewards)\n",
    "    p25_rewards = np.percentile(episode_rewards, 25)\n",
    "    median_rewards = np.median(episode_rewards)\n",
    "    p75_rewards = np.percentile(episode_rewards, 75)\n",
    "    max_rewards = np.max(episode_rewards)\n",
    "    \n",
    "    count_lengths = len(episode_lengths)\n",
    "    mean_lengths = np.mean(episode_lengths)\n",
    "    std_lengths = np.std(episode_lengths)\n",
    "    min_lengths = np.min(episode_lengths)\n",
    "    p25_lengths = np.percentile(episode_lengths, 25)\n",
    "    median_lengths = np.median(episode_lengths)\n",
    "    p75_lengths = np.percentile(episode_lengths, 75)\n",
    "    max_lengths = np.max(episode_lengths)\n",
    "    \n",
    "    print(\"\\nFinal Evaluation Performance Summary:\")\n",
    "    print(f\"Total Episodes Evaluated: {num_episodes}\\n\")\n",
    "    header = \"{:<22} {:>15} {:>20}\".format(\"Statistic\", \"Rewards\", \"Episode Lengths\")\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    print(\"{:<22} {:>15d} {:>20d}\".format(\"Count\", count_rewards, count_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Mean\", mean_rewards, mean_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Std Dev\", std_rewards, std_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Min\", min_rewards, min_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"25th Percentile\", p25_rewards, p25_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Median\", median_rewards, median_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"75th Percentile\", p75_rewards, p75_lengths))\n",
    "    print(\"{:<22} {:>15.2f} {:>20.2f}\".format(\"Max\", max_rewards, max_lengths))\n",
    "else:\n",
    "    print(\"No performance data was collected. Please verify the Collector configuration.\")\n",
    "\n",
    "# Close the evaluation environment.\n",
    "eval_env_ant.close()\n",
    "print(\"Evaluation completed and environment closed.\")\n",
    "\n",
    "try:\n",
    "    # Save the recorded frames as a video file.\n",
    "    video_ant_path = os.path.join(a2c_ant_agent_dir, f\"ant_v5_simulation_{agent_id}.mp4\")\n",
    "    imageio.mimsave(video_ant_path, frames_ant, fps=60)\n",
    "    print(f\"Ant simulation video saved as {video_ant_path}\")\n",
    "except Exception as e:\n",
    "    print(\"If a video is intended, make sure that the rendering mode is set to 'rgb_array' in the environment.\")\n",
    "    print(f\"Otherwise, an error occurred while saving the simulation video: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABE_tutorial_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
