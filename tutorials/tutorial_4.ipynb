{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABE Tutorial 4\n",
    "## Using Functional Approximation in Deep Reinforcement Learning\n",
    "\n",
    "In this tutorial, we explore how neural networks can be used for function approximation in deep reinforcement learning (DRL). We build on previous tutorials that covered value-based and actor–critic methods, and now we focus on:\n",
    "\n",
    "- **Constructing neural networks** using PyTorch.\n",
    "- **Data normalization** techniques to stabilize learning.\n",
    "- **Training networks** using gradient descent and backpropagation.\n",
    "- **Extending to continuous action spaces** for actor–critic methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 1. Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use PyTorch as our Python package to build neural networks. We've seen these before in our first four tutorials, but here let's dive into the details a little more.\n",
    "\n",
    "The first thing to note is that we are using a sequential approach to building our neural networks. In this approach we just need to specify a network by providing an ordered list of layers. Let's take a look at how to do this below, by building a simple three layer network:\n",
    "\n",
    "* **Input Layer**: This is the layer where the data comes into the model. Let's assume there are 4 input variables.\n",
    "\n",
    "* **First Hidden Layer**: This a layer of nodes that is connected to the input layer and will transform the input data, and pass these transformed values to the output layer. Let's assume this hidden layer has 32 nodes.\n",
    "\n",
    "* **Output Layer**: This output layer will take the transformed values and output values that can be used to inform what actions can be taken. Let's assume there are two actions that can be taken.\n",
    "\n",
    "You should see below these 3 layers, and you should see how each layers shape corresponds to the data: e.g., 4 input values gets passed to the 32 nodes in the hidden layer, and how those 32 nodes pass those transformed values to the 2 actions in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def build_simple_network():\n",
    "    \"\"\"\n",
    "    Construct a simple neural network.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Sequential: A neural network model.\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(4, 32),  # Maps 4 input features to 32 hidden nodes.\n",
    "        nn.Linear(32, 32), # Hidden layer maintains 32 features.\n",
    "        nn.Linear(32, 2)   # Output layer produces 2 values (e.g., action values).\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build the network and print its architecture.\n",
    "simple_net = build_simple_network()\n",
    "print(simple_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2. Inspecting the Network Architecture\n",
    "\n",
    "We use the `torchinfo` package (a modern alternative to `torchsummary`) to inspect the network. This tool provides detailed insights into each layer, including the shape of tensors and the number of trainable parameters. This step is essential for understanding how data flows through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Define the input shape as a tuple: batch size of 1 and 4 features.\n",
    "input_shape = (1, 4)\n",
    "\n",
    "# Display the network summary.\n",
    "summary(simple_net, input_size=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the summary above that the model has 1282 parameters! These are all the weights and bias values that are associated with each edge.\n",
    "\n",
    "However, linear layers alone can only model linear relationships. In DRL and deep learning, activation functions such as ReLU introduce non-linearity, enabling the network to approximate more complex functions. \n",
    "\n",
    "In our network:\n",
    "- **ReLU (Rectified Linear Unit):** Zeroes out negative values, helping to model non-linearities.\n",
    "\n",
    "**Note**: We do not apply an activation function to the output layer so that the network can output any continuous value, which is often necessary for value estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network_with_activation():\n",
    "    \"\"\"\n",
    "    Construct a neural network with activation functions.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Sequential: A neural network model with ReLU activations.\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(4, 32),\n",
    "        nn.ReLU(),             # Introduces non-linearity.\n",
    "        nn.Linear(32, 32),\n",
    "        nn.ReLU(),             # Second non-linear transformation.\n",
    "        nn.Linear(32, 2)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Create the network with activation functions.\n",
    "activated_net = build_network_with_activation()\n",
    "\n",
    "# Display the model summary using torchinfo.\n",
    "summary(activated_net, input_size=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By placing the ReLU activation layers after each layer we are filtering out any nodes that are outputting negative values. This cutoff is what let's a neural network model non-linear relationships.\n",
    "\n",
    "You'll notice that the model has the same number of weights and biases parameters. This is because the activation function is really just a filter and requires no new parameters. \n",
    "\n",
    "You'll notice too that there is no activation function applied after the output layer. This is because we want the output layer to output a continuous value and we want to keep negative values as an option. We'll see that for the output layer we have to think more about what kinds of outputs we want (continuous numeric, restricted to be between 0-1, ...etc) and that will determine how we build this last layer. Internally, however, with the hidden layers we will generally use ReLU activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3. Incorporating Normalization\n",
    "\n",
    "Normalization techniques, such as Layer Normalization, are crucial for stabilizing the training of deep networks. In DRL, where the agent’s experience is non-stationary, normalization helps maintain a consistent scale for inputs and intermediate activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_normalized_network():\n",
    "    \"\"\"\n",
    "    Construct a neural network with normalization layers.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Sequential: A normalized neural network model.\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(4, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.LayerNorm(32),   # Normalize activations from the first hidden layer.\n",
    "        nn.Linear(32, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.LayerNorm(32),   # Normalize activations from the second hidden layer.\n",
    "        nn.Linear(32, 2)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build and inspect the normalized network.\n",
    "normalized_net = build_normalized_network()\n",
    "summary(normalized_net, input_size=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LayerNorm:** Normalizes the activations of a layer for each given example, which is particularly useful in deep networks and recurrent models.\n",
    "\n",
    "Normalization layers add additional parameters for scaling and shifting but are omitted in the output layer to preserve the meaningful scale of outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 4. Simulating Data for Regression\n",
    "\n",
    "We've seen how we can build neural networks using a lego like approach and using different kinds of layers. Let's see now how we can update the weights/biases of these layers so that the network can learn. To do this, let's:\n",
    "\n",
    "* Simulate some data to use as input\n",
    "* Measure how far the network predictions are from the \"right\" answer\n",
    "* Adjust the weights and biases to make better predictions\n",
    "* Do this many times, until the network is making good predictions!\n",
    "\n",
    "To train our network, we simulate a simple regression problem where the state variable and expected return have a linear relationship with added noise:\n",
    "\n",
    "$$ y = 0.5x + \\epsilon $$\n",
    "\n",
    "where $\\epsilon$ represents Gaussian noise. This provides a controlled environment to study the learning process, loss minimization, and gradient-based optimization, which are foundational concepts in both deep learning and DRL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def simulate_data(num_samples=1000):\n",
    "    \"\"\"\n",
    "    Simulate a linear relationship between a state variable and an expected return.\n",
    "    \n",
    "    The relationship is given by:\n",
    "        y = 0.5 * x + ε\n",
    "    where ε is Gaussian noise.\n",
    "    \n",
    "    Args:\n",
    "        num_samples (int): Number of data samples.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Arrays of state variables and expected returns.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng()  # Modern random generator for reproducibility\n",
    "    state_vars = rng.normal(loc=0, scale=1, size=num_samples)\n",
    "    expected_return = state_vars * 0.5 + rng.normal(loc=0, scale=0.25, size=num_samples)\n",
    "    return state_vars, expected_return\n",
    "\n",
    "# Generate the synthetic data.\n",
    "state_vars, expected_return = simulate_data()\n",
    "\n",
    "# Plot the relationship between the state variable and expected return.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(state_vars, expected_return, alpha=0.6)\n",
    "plt.xlabel(\"State Variable\")\n",
    "plt.ylabel(\"Expected Return\")\n",
    "plt.title(\"Scatter Plot: State Variable vs Expected Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our nerual network can learn the relationship between state values and expected rewards. \n",
    "\n",
    "***\n",
    "## 5. Constructing a Regression Network\n",
    "\n",
    "For our regression problem, we design a network that takes a single input (state variable) and outputs a single prediction (expected return). The architecture includes:\n",
    "\n",
    "- **Input Layer:** 1 neuron.\n",
    "- **Hidden Layers:** Two layers with 32 neurons each, utilizing ReLU activations and Layer Normalization.\n",
    "- **Output Layer:** 1 neuron for continuous output.\n",
    "\n",
    "This design illustrates how even simple networks in DRL must be carefully architected to capture the underlying data distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regression_network():\n",
    "    \"\"\"\n",
    "    Build a neural network for regression with one input and one output.\n",
    "    \n",
    "    Returns:\n",
    "        nn.Sequential: The constructed regression network.\n",
    "    \"\"\"\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(1, 32),  # Input layer: 1 feature to 32 neurons.\n",
    "        nn.ReLU(),\n",
    "        nn.LayerNorm(32),\n",
    "        nn.Linear(32, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.LayerNorm(32),\n",
    "        nn.Linear(32, 1)   # Output layer: 1 continuous value.\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build the regression network and print its summary.\n",
    "regression_net = build_regression_network()\n",
    "summary(regression_net, input_size=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LayerNorm is a little over kill for the simple model we are building (i.e., it is more useful in deeper networks that are learning continuously), but let's leave it in for the example.\n",
    "\n",
    "Let's see how well this network does without learning (i.e., all the weights/biases are randomly selected).\n",
    "\n",
    "***\n",
    "## 6. Preparing Data for the Network\n",
    "\n",
    "Before training, we convert our simulated state variables into a PyTorch tensor that matches the input shape expected by our regression network. This is an important preprocessing step in DRL and deep learning in general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the state variables into a PyTorch tensor with shape (N, 1)\n",
    "state_vars_tensor = torch.tensor(state_vars, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Generate predictions using the untrained regression network.\n",
    "with torch.no_grad():\n",
    "    initial_predictions = regression_net(state_vars_tensor)\n",
    "\n",
    "# Plot the true data against the initial network predictions.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(state_vars, expected_return, label=\"True Data\", alpha=0.6)\n",
    "plt.scatter(state_vars, initial_predictions.numpy(), color=\"red\", label=\"Initial Predictions\", alpha=0.6)\n",
    "plt.xlabel(\"State Variable\")\n",
    "plt.ylabel(\"Expected Return\")\n",
    "plt.title(\"Untrained Model Predictions vs True Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may run the code multiple times to observe that the initial predictions vary due to random weight initialization. As expected, these predictions do not fit the data well before training.\n",
    "\n",
    "To quantify the discrepancy between the network's predictions and the true expected rewards, we use a **loss function**. The loss function provides a single scalar value that measures the error in the network's predictions and guides the learning process by indicating how much the network's parameters need to change.\n",
    "\n",
    "***\n",
    "## 7. Loss Function: Mean Squared Error\n",
    "\n",
    "In supervised learning and DRL, the loss function is critical for evaluating model performance. Here, we use the **Mean Squared Error (MSE)**, defined as:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$ is the number of samples.\n",
    "- $y_i$ is the true expected reward for the $i$-th sample.\n",
    "- $\\hat{y}_i$ is the predicted reward for the $i$-th sample.\n",
    "\n",
    "This metric computes the average of the squared differences between the actual and predicted values. The gradients of the MSE with respect to the network parameters are computed using the chain rule, enabling the optimizer to update the weights in a direction that minimizes the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the expected return data to a PyTorch tensor.\n",
    "expected_return_tensor = torch.tensor(expected_return, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define the MSE loss function.\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Compute the loss between initial predictions and the true expected returns.\n",
    "loss = loss_fn(initial_predictions, expected_return_tensor)\n",
    "print(f\"Initial Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions serve as the objective that our training process aims to minimize. They quantify the discrepancy between the model's predictions and the true targets, guiding the network to learn meaningful representations of the data.\n",
    "\n",
    "Let's see if we can reduce the MSE!\n",
    "\n",
    "***\n",
    "## 8. Optimizers\n",
    "\n",
    "**Optimizers** are algorithms designed to update the network’s parameters in order to minimize the loss function. They use gradient information computed with respect to the loss to determine the direction and magnitude of updates to the weights and biases. For example, the [Adam optimizer](https://arxiv.org/abs/1412.6980) uses adaptive learning rates and momentum to converge on a set of weights that reduce the loss. Its update rule is:\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t - \\alpha \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon},\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $w_t$ is the current weight,\n",
    "- $\\alpha$ is the learning rate,\n",
    "- $\\hat{m}_t$ is the bias-corrected first moment (mean of gradients),\n",
    "- $\\hat{v}_t$ is the bias-corrected second moment (variance of gradients),\n",
    "- $\\epsilon$ is a small constant to prevent division by zero.\n",
    "\n",
    "The optimizer uses the gradient information to update the weights in a direction that minimizes the loss (*gradient descent*), which in our case is the MSE.\n",
    "\n",
    "***\n",
    "## 9. Backpropagation\n",
    "\n",
    "**Backpropagation** is the method used to compute the gradients that the optimizer requires. It works in two main phases:\n",
    "\n",
    "1. **Forward Pass:**\n",
    "\n",
    "    a. *Neuron Computation:*  \n",
    "     Each neuron computes a weighted sum of its inputs plus a bias:\n",
    "     $$\n",
    "     z = \\sum_{j} w_j x_j + b,\n",
    "     $$\n",
    "     where:\n",
    "     - $x_j$ are the inputs,\n",
    "     - $w_j$ are the weights,\n",
    "     - $b$ is the bias.\n",
    "     \n",
    "     The neuron then applies an activation function (such as ReLU) to produce its output:\n",
    "     $$\n",
    "     a = \\sigma(z).\n",
    "     $$\n",
    "   \n",
    "    b. *Layer-by-Layer Propagation:*  \n",
    "     The outputs from one layer become the inputs to the next, culminating in the final prediction, $\\hat{y}$.\n",
    "   \n",
    "    c. *Loss Computation:*  \n",
    "     The MSE loss is computed over all samples:\n",
    "     $$\n",
    "     \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2,\n",
    "     $$\n",
    "     where:\n",
    "     - $N$ is the number of samples,\n",
    "     - $y_i$ is the true value for the $i$-th sample,\n",
    "     - $\\hat{y}_i$ is the predicted value.\n",
    "\n",
    "2. **Backward Pass:**\n",
    "\n",
    "    a. *Local Gradient Computation:*  \n",
    "     At the neuron level, we calculate the derivative of the activation function, $\\sigma'(z)$, to understand how a small change in $z$ affects the output $a$.\n",
    "     \n",
    "    b. *Applying the Chain Rule:*  \n",
    "     When computing the gradient of the loss $L$ with respect to a weight $w$, we use the chain rule, which is\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}\n",
    "     $$\n",
    "     Here:\n",
    "     - $\\frac{\\partial L}{\\partial \\hat{y}}$ tells us how the loss changes as the network's output $\\hat{y}$ changes. It is derived by differentiating the loss function with respect to the network's prediction.\n",
    "     - $\\frac{\\partial \\hat{y}}{\\partial z}$ is the derivative of the activation function. It measures how the neuron's output $\\hat{y}$ responds to a change in its input $z$. For example, if you use the ReLU activation, this derivative is 1 when $z$ is positive and 0 when $z$ is not positive.\n",
    "     - $\\frac{\\partial z}{\\partial w}$ is simply the input $x$, since the neuron's input is computed as $z = w \\cdot x + b$. This indicates that a small change in $w$ results in a change in $z$ proportional to $x$.\n",
    "\n",
    "   \n",
    "    c. *Gradient Propagation:*  \n",
    "     These gradients are computed starting from the output layer and propagated backward through the network. This tells us how each weight contributes to the overall MSE loss.\n",
    "\n",
    "The optimizer then uses these computed gradients to update the weights, aiming to reduce the loss. By repeating this cycle, the network incrementally improves its predictions, driving the MSE lower with each *epoch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer using the Adam algorithm with a learning rate of 0.01.\n",
    "optimizer = torch.optim.Adam(regression_net.parameters(), lr=0.01)\n",
    "\n",
    "# Forward pass: Compute predictions using the regression network.\n",
    "predictions = regression_net(state_vars_tensor)\n",
    "loss = loss_fn(predictions, expected_return_tensor)\n",
    "\n",
    "# Backpropagation and optimization steps.\n",
    "optimizer.zero_grad()   # Clears any existing gradients.\n",
    "loss.backward()         # Computes the new gradients using the chain rule.\n",
    "optimizer.step()        # Applies the computed gradients to update the model's weights.\n",
    "\n",
    "print(f\"New loss after another training step: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the above code cell above a few times. Hopefully, you see the loss get smaller than the first loss!\n",
    "\n",
    "In practice, the network is trained over many iterations (epochs). During each epoch, the following occurs:\n",
    "1. **Forward Pass:** Compute predictions for all input data.\n",
    "2. **Loss Calculation:** Evaluate the MSE loss.\n",
    "3. **Backpropagation:** Use the chain rule to compute gradients.\n",
    "4. **Weight Update:** Adjust weights using gradient descent (Adam optimizer).\n",
    "\n",
    "This iterative process minimizes the loss, allowing the network to approximate the target function. The theory behind this process stems from optimization theory and the principles of stochastic gradient descent, which underlie many DRL algorithms.\n",
    "\n",
    "Let's formalize these steps a little better, and write the steps into a loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, optimizer, loss_fn, inputs, targets, num_epochs=1000, log_interval=100):\n",
    "    \"\"\"\n",
    "    Train the neural network model using a training loop.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The neural network to be trained.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model parameters.\n",
    "        loss_fn (nn.Module): Loss function to compute prediction error.\n",
    "        inputs (torch.Tensor): Input data.\n",
    "        targets (torch.Tensor): Target data.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        log_interval (int): Interval (in epochs) to log the loss.\n",
    "    \n",
    "    Returns:\n",
    "        list: History of loss values logged during training.\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass: compute predictions.\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        \n",
    "        # Backpropagation: reset gradients, compute new gradients, and update weights.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the loss at specified intervals.\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "            loss_history.append(loss.item())\n",
    "    \n",
    "    return loss_history\n",
    "\n",
    "# Train the regression network using the defined training loop.\n",
    "loss_history = train_network(regression_net, optimizer, loss_fn, state_vars_tensor, expected_return_tensor, num_epochs=1000, log_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 10. Evaluating Model Performance\n",
    "\n",
    "After training the model over multiple epochs, it is important to evaluate the network’s performance by comparing the network's final predictions to the true data. The goal is for the predicted values to align closely with the target values.\n",
    "\n",
    "Since we use neural networks as function approximators in DRL, we need it to accurately represent the value or policy function. The closer the predicted values are to the true expected rewards, the better the agent can learn and make decisions.\n",
    "\n",
    "In our DRL context, we can think of $y_i$ as the expected reward for a given state $s_i$, and $\\hat{y}_i$ as the predicted reward from the neural network.\n",
    "\n",
    "A close alignment between $y_i$ and $\\hat{y}_i$ indicates that the network has learned the underlying relationship in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final predictions without computing gradients.\n",
    "with torch.no_grad():\n",
    "    final_predictions = regression_net(state_vars_tensor)\n",
    "\n",
    "# Plot the true data vs. the model's final predictions.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(state_vars, expected_return, label=\"True Data\", alpha=0.6)\n",
    "plt.scatter(state_vars, final_predictions.numpy(), color=\"red\", label=\"Predicted Data\", alpha=0.6)\n",
    "plt.xlabel(\"State Variable\")\n",
    "plt.ylabel(\"Expected Return\")\n",
    "plt.title(\"Model Predictions vs True Data After Training\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see that the model predictions are much better aligned with the true expected returns! \n",
    "\n",
    "Note: we know the true relationship is a simple linear line with 0.5 slope. But we can see that the red line is trying to find more complex relationships, and is overfitting the data! There are ways to minimize overfitting of a neural network model and we'll see some of these as we go through some more RL examples. But it is usually a good idea to overfit our models first then take steps to reduce overfiting (e.g., regularize)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "From the examples above we should now have an introductory sense of how we can build and train neural network models! We'll build on these skills in these tutorials going forward. The first thing we'll do is see how we can change the outputs of our neural networks to allow for continuous actions! We'll do this in the next tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABE_tutorial_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
